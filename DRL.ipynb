{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8rcBAvYsf6h"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/plots \n",
        "!mkdir /content/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iua6xdK2vMb3"
      },
      "outputs": [],
      "source": [
        "class User_initialization(object):\n",
        "    def __init__(self, user_config, train_config):\n",
        "        self.id = user_config['id']\n",
        "        self.M_u = user_config['M_u']\n",
        "        self.P_u = user_config['P_u']\n",
        "        self.F_u = user_config['F_u']\n",
        "        self.t_factor = user_config['t_factor']\n",
        "        self.penalty = user_config['penalty']\n",
        "        self.sigma2 = train_config['sigma2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2KRgW6TvOzF"
      },
      "outputs": [],
      "source": [
        "class Server_initialization(object):\n",
        "    def __init__(self, server_config, train_config):\n",
        "        self.id = server_config['id']\n",
        "        self.rate = server_config ['id']\n",
        "        self.dis = server_config['dis']\n",
        "        self.F_e = server_config['F_e']\n",
        "        self.t_factor = server_config['t_factor']\n",
        "        self.penalty = server_config['penalty']\n",
        "        self.sigma2 = train_config['sigma2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdLTOPoezaLk"
      },
      "outputs": [],
      "source": [
        "class MecSvrEnv(object):\n",
        "    \"\"\"\n",
        "    Simulation environment\n",
        "    \"\"\"\n",
        "    def __init__(self, user_list, server_list, sigma2): \n",
        "          self.user_list = user_list\n",
        "          self.server_list = server_list\n",
        "          self.num_user = len(user_list)\n",
        "          self.num_server = len(server_list)\n",
        "          self.sigma2 = sigma2\n",
        "          self.count = 0\n",
        "          alpha = -3 \n",
        "          self.time_slot = 0\n",
        "          self.seqCount = 0\n",
        "          self.transmission = np.zeros(self.num_server)\n",
        "          self.Computing_server = np.zeros(self.num_server)\n",
        "          self.feedback = np.zeros(self.num_server)\n",
        "          self.cellular_power = 0.2 # W\n",
        "          distance_cellular = 0.3\n",
        "          alpha = -3\n",
        "          cell = []\n",
        "          for i in range (2):\n",
        "            X =(1/math.sqrt(2))*(pow(distance_cellular, alpha)* np.random.randn())\n",
        "            Y =(1/math.sqrt(2))*(pow(distance_cellular, alpha)* np.random.randn())\n",
        "            h= math.sqrt(pow(X,2) + pow(Y,2)) \n",
        "            cell.append(h)\n",
        "          \n",
        "          self.cellular_channel = cell[1]\n",
        "          #self.cellular_power = np.random.randint(1,100)\n",
        "          #self.cellular_channel=np.random.randint(1,50)\n",
        "          self.f_user = np.random.randint(50,200,size=(1))# 1 egal a num_users cycles per second\n",
        "\n",
        "          #self.f_servers = np.random.randint(100,500,size=( self.num_server)) \n",
        "          self.f_servers = np.zeros(self.num_server) # cycles per second\n",
        "          self.f_servers[0]=600\n",
        "          self.f_servers[1]=500\n",
        "          self.f_servers[2]=500\n",
        "          self.f_servers[3]=400\n",
        "          self.f_servers[4]=300\n",
        "          self.c = 20 # cycles per second\n",
        "          #self.channels = np.random.randint(1,5,size=(self.num_server)) # h est le channel\n",
        "          #self.channels = np.random.uniform(10,15,self.num_server) # h est le channel\n",
        "          self.distances = [0.2,0.7,0.5,0.5,0.3,0.3]\n",
        "          self.channels = []\n",
        "          number = self.num_server + 1\n",
        "          for i in range(number):\n",
        "            X =(1/math.sqrt(2))*(pow(self.distances[i], alpha)* np.random.randn())\n",
        "            Y =(1/math.sqrt(2))*(pow(self.distances[i], alpha)* np.random.randn())\n",
        "            h = math.sqrt(pow(X,2) + pow(Y,2)) \n",
        "            self.channels.append(h)\n",
        "          self.channels.pop(0)\n",
        "          #print('channels',self.channels)\n",
        "          #self.Powers = np.random.randint(10,30,size=( self.num_server))\n",
        "          self.Powers = np.random.uniform(0,1,self.num_server)\n",
        "          #print('self.Powers', self.Powers)\n",
        "          self.SINR = 0\n",
        "          self.Reward = 0         \n",
        "          self.B = 10  #MHZ\n",
        "          self.downlinks_Data_rates = np.zeros(shape = (self.num_server))\n",
        "          local_CPU =0\n",
        "          self.Data_rates = 0\n",
        "          self.f_allocated = 0\n",
        "          self.f_allocated_local = 0\n",
        "          self.sinr_list = np.zeros(shape = (self.num_server))\n",
        "          self.snr_list = np.zeros(shape = (self.num_server))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_channels(self):\n",
        "      channels = []\n",
        "      alpha = -3 \n",
        "      number = self.num_server + 1\n",
        "      for i in range(number):\n",
        "        X =(1/math.sqrt(2))*(pow(self.distances[i], alpha)* np.random.randn())\n",
        "        Y =(1/math.sqrt(2))*(pow(self.distances[i], alpha)* np.random.randn())\n",
        "        h = math.sqrt(pow(X,2) + pow(Y,2)) \n",
        "        channels.append(h)\n",
        "      channels.pop(0)\n",
        "      #print('channels',channels)\n",
        "      return channels\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def find(self,element, vector):\n",
        "      for i in range(len(vector)):\n",
        "        if vector[i]== element:\n",
        "            return i    \n",
        "        \n",
        "        \n",
        "    def getCh(self):\n",
        "        ch = np.random.choice(self.channels)\n",
        "        server_id = self.find(ch,self.channels)\n",
        "        return ch , server_id\n",
        "\n",
        "\n",
        "\n",
        "    def Task_received_server(self, server_id, Task_size):\n",
        "      self.Task_size = Task_size\n",
        "      \n",
        "      if server_id == 0:\n",
        "        #self.f_allocated = np.random.randint(500,600)\n",
        "        self.f_allocated = 550\n",
        "      elif server_id == 1:\n",
        "      #self.f_allocated = np.random.randint(300,400)\n",
        "        self.f_allocated = 350\n",
        "\n",
        "      elif server_id == 2:\n",
        "        #self.f_allocated = np.random.randint(400,500)\n",
        "        self.f_allocated = 450\n",
        "\n",
        "      elif server_id == 3:\n",
        "        #self.f_allocated = np.random.randint(300,400)\n",
        "        self.f_allocated = 350\n",
        "\n",
        "      elif server_id == 4:\n",
        "        #self.f_allocated = np.random.randint(200,300)\n",
        "        self.f_allocated = 250\n",
        "\n",
        "      #f_allocated = self.c * self.Task_size\n",
        "      return self.f_allocated\n",
        "\n",
        "\n",
        "    def reset_f_servers(self, f_servers, server_id, sub_task ):\n",
        "        self.f_servers = f_servers\n",
        "        self.server_id = server_id\n",
        "            #f(t) is a vector of remaining CPU - cycles frequency\n",
        "        servers = self.f_servers[self.server_id]\n",
        "        self.f_allocated = self.Task_received_server(self.server_id,self.sub_task ) \n",
        "        self.f_servers[self.server_id] = servers - self.f_allocated\n",
        "        #print('nhina ', self.f_allocated,  \"men \",servers)   \n",
        "        #print('haw chel9ina', self.f_servers[self.server_id] )    \n",
        "        if  self.f_servers[self.server_id] <0:\n",
        "          self.f_servers[self.server_id] =1\n",
        "        return self.f_servers, self.f_allocated\n",
        "\n",
        "    def reset_f_servers_inverse(self,f_servers, server_id, f_allocated ):\n",
        "        self.f_servers = f_servers\n",
        "        self.server_id = server_id  \n",
        "        self.f_allocated =   f_allocated\n",
        "\n",
        "        self.f_servers[self.server_id] = self.f_servers[self.server_id] + self.f_allocated\n",
        "        return self.f_servers\n",
        "\n",
        "\n",
        "    def Task_exec_local(self,Task_size):\n",
        "        self.Task_size = Task_size\n",
        "        self.f_allocated_local= np.random.randint(2,20)\n",
        "\n",
        "        #f_allocated_local= np.random.randint(2,10)* self.Task_size\n",
        "        return self.f_allocated_local\n",
        " \n",
        "    def reset_f_users(self, f_user, Task_size ):\n",
        "        self.f_user = f_user\n",
        " \n",
        "            #f(t) is a vector of remaining CPU - cycles frequency\n",
        "        self.f_user = self.f_user - self.Task_exec_local(Task_size)       \n",
        "        return self.f_user\n",
        "    \n",
        "    def reset_f_users_inverse(self, f_user,  f_allocated_local):\n",
        "        self.f_user = f_user\n",
        "        self.f_allocated_local = f_allocated_local\n",
        "        self.f_user= self.f_user + self.f_allocated_local    \n",
        "        return self.f_user\n",
        "\n",
        "\n",
        "  # \n",
        "    def transmission_computing_server(self, server_id, f_allocated, sub_task, power, channels):\n",
        "        self.channels = channels\n",
        "        channel_server = self.channels[server_id]   \n",
        "        self.sub_task = sub_task\n",
        "        self.power = power\n",
        "        no_executing = False\n",
        "        self.f_allocated = f_allocated\n",
        "        data_rates = self.uplink_data_rate(server_id,channel_server, power)\n",
        "        #for i in range(self.num_user):\n",
        "        \n",
        "        data_rate_user = data_rates\n",
        "        #print('data_rate_user', data_rate_user)\n",
        "        #print('data_rates[server_id]', data_rates)\n",
        "        if data_rates!= 0.0:\n",
        "        #self.f_servers = self.reset_f_servers(self.f_servers, server_id, self.sub_task )\n",
        "        #print('fservers',self.f_servers)\n",
        "          self.transmission[server_id] = self.sub_task  / data_rate_user\n",
        "        #self.f_allocated = self.Task_received_server( server_id, self.sub_task)\n",
        "        #self.Computing_server[server_id]= (self.c * self.sub_task )/ self.f_servers[server_id]\n",
        "      \n",
        "          self.Computing_server[server_id]= (self.c * self.sub_task )/ self.f_allocated\n",
        "        else :\n",
        "          no_executing = True\n",
        "          \n",
        "          #return array of transmission delay for each user for server _id\n",
        "        return self.transmission[server_id] , self.Computing_server[server_id], self.f_servers, no_executing\n",
        "\n",
        "    def predict(self,action):\n",
        "      #action c'est un vecteur de 3 valeurs \n",
        "        self.action = action\n",
        "        #for one user\n",
        "        Tasks = [50,50,30,30,20]\n",
        "        powers = [0.5,0.4,0.3,0.2,0.1]\n",
        "        self.offloading_server = self.action # the server id\n",
        "        self.sub_task = Tasks[self.action]\n",
        "        self.power = powers[self.action]\n",
        "        return self.offloading_server, self.sub_task, self.power\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def local_computing(self, M_t ):\n",
        "      '''\n",
        "        sub_tasks_size = 0\n",
        "        offloading_server,sub_task,_ = self.predict()\n",
        "        #for (decision_idx), value in np.ndenumerate(offloading_server) :\n",
        "        if offloading_server == 0:\n",
        "            sub_tasks_size = sub_tasks_size + sub_task\n",
        "        if offloading_server == 1:\n",
        "            sub_tasks_size = sub_tasks_size + sub_task\n",
        "\n",
        "        local_sub_task = M_t - sub_tasks_size\n",
        "      '''\n",
        "      \n",
        "      local_sub_task = M_t\n",
        "      #self.f_user=self.reset_f_users( self.f_user, local_sub_task) \n",
        "      #self.f_allocated_local = self.Task_exec_local(local_sub_task) \n",
        "      local_CPU = local_sub_task / 2\n",
        "       \n",
        "      #we have used more than all resources => some informations are lost:\n",
        "      #self.f_user =self.reset_f_users_inverse(self.f_user, self.f_allocated_local)\n",
        "      \n",
        "      #only the local computing value\n",
        "      return local_CPU\n",
        "\n",
        "\n",
        "                  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # we are considering only one edge server here\n",
        "  # num.user should be the number of users offloading to edge server e, \n",
        "  # we are going to take all the users because when one user doesn't offload to edge server e \n",
        "  # it is already 0\n",
        "  # powers is the determinated powers for each edge server\n",
        "  # channels is the h from user u to  edge servers\n",
        "\n",
        "    def compute_sinr(self,server_id, channels_0, powers_0):\n",
        "      # Power-Domain NOMA \n",
        "      # calculate the received power at the MEC server for each user\n",
        "      #channel_gains = np.power(np.linalg.norm(channels_0), 2)\n",
        "      #print('powers_0', powers_0)\n",
        "      channel_gains = np.power(channels_0, 2)\n",
        "      #print('channel_gains', channel_gains)\n",
        "      receive_powers = channel_gains * powers_0 \n",
        "      #print('receive_powers', receive_powers)\n",
        "  \n",
        "      #receive_powers = pow(channel_gains,powers_0) \n",
        "      cellular_p = self.cellular_power * self.cellular_channel\n",
        "      #print('cellular_power', self.cellular_power)\n",
        "      #print('cellular_channel', self.cellular_channel)\n",
        "      #print('cellular_p', cellular_p)\n",
        "      #total_power = np.sum(receive_powers)\n",
        "\n",
        "  #         # ordering the channels by their power gain in an acending order\n",
        "      #idx_list = np.argsort(receive_powers)[::-1]\n",
        "\n",
        "  #         # get access to the channel and decode in an decending order\n",
        "      #sinr_list = []\n",
        "      num_user = np.random.randint(5)\n",
        "\n",
        "      #all other users offloading to edge server\n",
        "      total_power =0\n",
        "      if num_user != 0:\n",
        "        for i in range(self.num_user):\n",
        "          total_power = total_power +  np.random.uniform(0,1) * np.random.uniform(0,5)\n",
        "\n",
        "      #print('total_power' , total_power)\n",
        "      #for i in range(self.num_user):\n",
        "      #total_power -= receive_powers[user_idx]\n",
        "\n",
        "      result= receive_powers /(total_power + cellular_p + self.sigma2) \n",
        "      if result > 1 and result < 1000:\n",
        "        self.sinr_list[server_id] = result\n",
        "      \n",
        "      #sinr_list[user_idx] = receive_powers[user_idx]/(total_power+cellular_p+self.sigma2)  \n",
        "      #print('sinr_list',self.sinr_list )\n",
        "\n",
        "      return self.sinr_list\n",
        "  #sinr is affected for each user correctly. I can access sinr_list using user_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # datarate for ech user offloading to edge server e => array of data rates for server e \n",
        "    def uplink_data_rate(self,server_id, channels_0, powers_0):\n",
        "        #data rate from each user u to edge server e\n",
        "        self.channels_0 = channels_0\n",
        "        self.powers_0 = powers_0\n",
        "        self.server_id = server_id\n",
        "        sinr_list = self.compute_sinr(self.server_id, self.channels_0, self.powers_0)\n",
        "        #print('sinr_list', sinr_list)\n",
        "        #self.Data_rates = np.zeros(shape=(1))\n",
        "        \n",
        "        #for i in range(self.num_user):\n",
        "        #self.Data_rates = np.append(self.Data_rates,self.B*math.log(sinr_list[user_idx],2))\n",
        "        if sinr_list[server_id] != 0:\n",
        "          self.Data_rates = self.B*log(sinr_list[server_id]+1,2)\n",
        "        \n",
        "        return self.Data_rates\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # from servers to users yaaani hatha nhotou au niveau users bech nkharej feedback w nehseb reward au niveau e system de simulation \n",
        "    def compute_snr(self,server_id, channels_0, powers_0):\n",
        "        # calculate the received power at the MEC server for each user\n",
        "        channel_gains = np.power(channels_0, 2)\n",
        "        #channel_gains = np.power(np.linalg.norm(channels_0, axis=0), 2)\n",
        "        receive_powers = channel_gains * powers_0 \n",
        "     \n",
        "        #receive_powers = pow(channel_gains,powers_0)  \n",
        "        \n",
        "  #         # ordering the channels by their power gain in an acending order\n",
        "\n",
        "  #         # get access to the channel and decode in an decending order\n",
        "             \n",
        "        #for server_idx, value in np.ndenumerate(self.server_list) :\n",
        "        self.snr_list[server_id]  = receive_powers/(self.sigma2) \n",
        "        return self.snr_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #snr is affected for each server at the user cause it is received at the user side . \n",
        "    def downlink_data_rate(self, offloading_server, channels_0, powers_0):\n",
        "        self.channels_0 = channels_0\n",
        "        num_users = 6\n",
        "        self.powers_0 = powers_0\n",
        "        self.snr_list = self.compute_snr(offloading_server,self.channels_0, self.powers_0)\n",
        "        #for server_idx, value in np.ndenumerate(self.server_list) :\n",
        "        self.downlinks_Data_rates[offloading_server] = (self.B/num_users)*log(self.snr_list[offloading_server]+1,2)\n",
        "        return self.downlinks_Data_rates\n",
        "\n",
        "    def feed_back_user(self, offloading_server,f_allocated,sub_task,power, channels):\n",
        "        transmission_user = 0\n",
        "        #computing_user = 0\n",
        "        processed_tasks = 0  \n",
        "        feedback = 0  \n",
        "        self.channels = channels\n",
        "        self.f_allocated = f_allocated\n",
        "        channels_user = self.channels[offloading_server]\n",
        "        self.power = power\n",
        "        self.sub_task = sub_task  \n",
        "        self.downlinks_Data_rates = self.downlink_data_rate(offloading_server,channels_user, self.power)\n",
        "        #transmission_user, computing_user, self.f_servers  = self.transmission_computing_server(offloading_server,self.f_allocated, sub_task, power )\n",
        "        #processed tasks est plus petite\n",
        "        processed_tasks = self.sub_task / 10\n",
        "        feedback = processed_tasks/ self.downlinks_Data_rates[offloading_server]\n",
        "        #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.sub_task )\n",
        "\n",
        "        return feedback, self.f_servers\n",
        "\n",
        "    def reset(self, isTrain, Task, channels, offloading_matrix):\n",
        "          self.count = 0\n",
        "          self.channels = channels\n",
        "          self.offloading_matrix = offloading_matrix\n",
        "          self.Task = Task\n",
        "          if isTrain:\n",
        "              #initialize the tasks size M(t)\n",
        "              #M_t[user_id] =  user.set_m_t()\n",
        "              M_t =  self.set_m_t()\n",
        "              #initialize the f(t)\n",
        "              offloading_server,sub_task,power= self.predict()\n",
        "              ##f_servers = self.reset_f_servers(f_servers, offloading_server, sub_task )\n",
        "              #self.f_servers = np.random.randint(100,500,size=( self.num_server))\n",
        "              #initialize the offloading matrix x(t)\n",
        "              # offloading matrix here is a vector of 2 elements \n",
        "              offloading_matrix = self.reset_x_server(self, X, time_slot, offloading_server)      \n",
        "              #on lui ajout m_t\n",
        "              #offloading_matrix .append(M_t)\n",
        "              # puis on ajoute f_servers\n",
        "              #offloading_matrix = np.cancatenate(offloading_matrix, f_servers ) \n",
        "              #offloading_matrix is a vector of 5 elements\n",
        "              self.setState(offloding_matrix, M_t, self.f_servers)\n",
        "          else:\n",
        "              #initialize the tasks size M(t)\n",
        "              #initialize the offloading matrix x(t)\n",
        "              # ======> intiialize it to 0  \n",
        "\n",
        "              State = self.initialise_state(self.Task,self.channels, self.offloading_matrix)\n",
        "              isTrain = True\n",
        "              \n",
        "          self.time_slot += 1\n",
        "          return State\n",
        "\n",
        " # We update the X(t) for one server while looking for all the users.      \n",
        "    def reset_x_server(self, offloading_matrix, server_idx):\n",
        "\n",
        "      if self.server_connected(server_idx)== True:\n",
        "          offloading_matrix[server_idx]=1\n",
        "        \n",
        "      return offloading_matrix\n",
        "\n",
        "#reset est utilisÃ© apres chaque serveur choisit\n",
        "    def reset_m_t(self, State, time_slot, sub_task , num_server):\n",
        "      self.num_server = num_server\n",
        "      self.State = State\n",
        "      M_t = self.State[num_server]\n",
        "      if M_t > 0:\n",
        "        M_t = M_t - sub_task\n",
        "      self.State[num_server] = M_t\n",
        "      return self.State\n",
        "\n",
        "\n",
        "\n",
        " ### initialise state matrix au debut        \n",
        "    def initialise_state(self,Task, channels, offloading_matrix):  \n",
        "      self.channels = channels\n",
        "      self.offloading_matrix = offloading_matrix\n",
        "      self.Task = Task\n",
        "      State = []   \n",
        "      #offloading_matrix =  np.random.randint(2, size=(5))\n",
        "      M_t = self.Task\n",
        "      offloading_matrix = np.append(self.offloading_matrix,M_t)\n",
        "      #self.f_servers = self.f_servers\n",
        "      State= np.concatenate((offloading_matrix , self.channels))\n",
        "      #State= np.concatenate((State , self.channels))\n",
        "      State = State.astype(float)\n",
        "      print(State)\n",
        "      return State\n",
        "\n",
        "\n",
        " ### initialise action matrix au debut   \n",
        "    def initialise_action(self): \n",
        "       powers = [0.1,0.2,0.3,0.4]\n",
        "       sub_tasks = [20,30,40,50]\n",
        "       offloading = [0,1,2,3,4]\n",
        "       action = []\n",
        "       offloading_server = np.random.choice(offloading) \n",
        "       action.append(offloading_server)\n",
        "       sub_task = np.random.choice(sub_tasks)\n",
        "       action.append(sub_task)\n",
        "       power = np.random.choice(powers)\n",
        "       action.append(power)\n",
        "       return action\n",
        "\n",
        "\n",
        "\n",
        " ### initialise action matrix au debut   \n",
        "    def initialise_servers(self): \n",
        "       action = []\n",
        "       action = np.random.randint(0,2, size=5)\n",
        "       return action\n",
        "\n",
        "\n",
        " ### initialise action matrix au debut   \n",
        "    def initialise_Tasks(self): \n",
        "       sub_tasks = [20,30,40,50]\n",
        "       action = []\n",
        "       for i in range(5):\n",
        "        sub_task = np.random.choice(sub_tasks)\n",
        "        action.append(sub_task)\n",
        "       return action\n",
        "\n",
        "    def initialise_powers(self): \n",
        "       powers = [0.1,0.2,0.3,0.4]\n",
        "       action = []\n",
        "       for i in range(5):\n",
        "        power = np.random.choice(powers)\n",
        "        action.append(power)\n",
        "       return action\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def feedback(self,offloading_matrix , users_tasks, f_servers, rewards, done):\n",
        "        #rewards is a vector of reward for each time slot T \n",
        "        # we are going to calculate the cumulative rewrard\n",
        "        self.rewards = rewards\n",
        "        # a matrix of offloading \n",
        "        self.offloading_matrix = offloading_matrix\n",
        "        # tasks of each user \n",
        "        self.user_tasks = users_tasks\n",
        "        # available resources at edge servers\n",
        "        self.f_servers = f_servers\n",
        "        self.f_servers.append(0) \n",
        "        self.Reward = self.dis_factor*np.sum(self.rewards)\n",
        "        State_i = np.concatenate((self.offloading_matrix , self.users_tasks),axis=1)\n",
        "        next_state = np.concatenate((State_i , self.f_servers),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# AGENNNNNNNNNNNNNT\n",
        "\n",
        "# server_connection method is used to verify that one user is connected to one server\n",
        "    def server_connected(self, server_idx):\n",
        "      [ch, server_id] = self.getCh()\n",
        "      if server_id == server_idx:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "\n",
        "    def set_m_t(self):\n",
        "      #m_t = np.random.randint(50,100)\n",
        "      m_t = 100\n",
        "      return m_t\n",
        "\n",
        "    def setState(self, offloding_matrix, M_t, f_servers):\n",
        "      # x(t) is the offloading matrix\n",
        "      self.offloading_matrix = offloding_matrix\n",
        "      self.M_t = M_t\n",
        "      self.f_servers = f_servers\n",
        "      self.offloading_matrix = np.append(self.offloading_matrix,self.M_t)\n",
        "      self.State = np.concatenate((self.offloading_matrix, self.f_servers),axis=0)\n",
        "      return self.State\n",
        "\n",
        "    def step(self, action, observation, channels):\n",
        "      self.channels = channels\n",
        "      [server1, server2,server3,server4, server5, Task,  ch1, ch2, ch3, ch4, ch5] =  observation # th := theta\n",
        "      f1 = 550\n",
        "      f2 = 350\n",
        "      f3 = 450\n",
        "      f4 = 350\n",
        "      f5 = 250\n",
        "\n",
        "      local = 0\n",
        "      if Task > 0:\n",
        "        done = False\n",
        "        \n",
        "        offloading_server, sub_task, power= self.predict(action)\n",
        "\n",
        "        sub_task = min(Task, sub_task )\n",
        "        observation[5]= Task - sub_task\n",
        "        Task = observation[5]\n",
        "        if offloading_server == 0 and observation[0] == 1:     \n",
        "          print('Task', Task)\n",
        "          #self.f_servers, self.f_allocated = self.reset_f_servers(self.f_servers, offloading_server, sub_task )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "          self.transmission[offloading_server] ,self.Computing_server[offloading_server], self.f_servers, no_executing= self.transmission_computing_server(offloading_server,f1,sub_task, power, self.channels)\n",
        "          if no_executing == False:\n",
        "            self.feedback[offloading_server], self.f_servers = self.feed_back_user(offloading_server,self.f_allocated,sub_task,power, self.channels)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "          else :\n",
        "            local = local + self.local_computing(sub_task)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "        elif offloading_server == 1 and observation[1] == 1:\n",
        "\n",
        "          #print(observation[1])\n",
        "          #self.f_servers, self.f_allocated = self.reset_f_servers(self.f_servers, offloading_server, sub_task )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "          self.transmission[offloading_server] ,self.Computing_server[offloading_server], self.f_servers, no_executing= self.transmission_computing_server(offloading_server, f2, sub_task, power, self.channels )\n",
        "          if no_executing ==False:\n",
        "            self.feedback[offloading_server], self.f_servers= self.feed_back_user(offloading_server,self.f_allocated,sub_task,power, self.channels)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated)\n",
        "          else:\n",
        "            local =local + self.local_computing(sub_task)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "        elif offloading_server == 2 and observation[2] == 1:\n",
        "\n",
        "          #print(observation[2])\n",
        "          #self.f_servers, self.f_allocated = self.reset_f_servers(self.f_servers, offloading_server, sub_task )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "          self.transmission[offloading_server] ,self.Computing_server[offloading_server], self.f_servers, no_executing= self.transmission_computing_server(offloading_server, f3, sub_task, power, self.channels )\n",
        "          if no_executing ==False:\n",
        "            self.feedback[offloading_server], self.f_servers= self.feed_back_user(offloading_server,self.f_allocated,sub_task,power, self.channels)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated)\n",
        "          else:\n",
        "            local =local + self.local_computing(sub_task)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "        elif offloading_server == 3 and observation[3] == 1:\n",
        "\n",
        "          #print(observation[3])\n",
        "          #self.f_servers, self.f_allocated = self.reset_f_servers(self.f_servers, offloading_server, sub_task )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "          self.transmission[offloading_server] ,self.Computing_server[offloading_server], self.f_servers, no_executing= self.transmission_computing_server(offloading_server, f4, sub_task, power,self.channels)\n",
        "          if no_executing ==False:\n",
        "            self.feedback[offloading_server], self.f_servers= self.feed_back_user(offloading_server,self.f_allocated,sub_task,power,self.channels)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated)\n",
        "          else:\n",
        "            local =local + self.local_computing(sub_task)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "        elif offloading_server == 4 and observation[4] == 1:\n",
        "\n",
        "          #print(observation[4])\n",
        "          #self.f_servers, self.f_allocated = self.reset_f_servers(self.f_servers, offloading_server, sub_task )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "          self.transmission[offloading_server] ,self.Computing_server[offloading_server], self.f_servers, no_executing= self.transmission_computing_server(offloading_server, f5, sub_task, power,self.channels )\n",
        "          if no_executing == False:\n",
        "            self.feedback[offloading_server], self.f_servers= self.feed_back_user(offloading_server,self.f_allocated,sub_task,power, self.channels)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated)\n",
        "          else:\n",
        "            local =local + self.local_computing(sub_task)\n",
        "            #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )        \n",
        "        else:\n",
        "          local = local + self.local_computing(sub_task)\n",
        "          #self.f_servers = self.reset_f_servers_inverse(self.f_servers, offloading_server, self.f_allocated )\n",
        "          #print('self.f_servers',self.f_servers)\n",
        "\n",
        "          \n",
        "        self.offloading_matrix = observation[0:5]\n",
        "\n",
        "      else:\n",
        "        #print('task', Task)\n",
        "        done = True\n",
        "        offloading_server = -1\n",
        "      #if Task > 0:\n",
        "        #for server_idx in range(5):\n",
        "          #self.offloading_matrix = self.reset_x_server(self.offloading_matrix, server_idx)     \n",
        "        #self.State = self.setState(self.offloading_matrix, observation[5], self.f_servers)\n",
        "    \n",
        "      observation_ = observation\n",
        "      if Task == 0:\n",
        "        done= True\n",
        "\n",
        "      return observation_, self.transmission, self.Computing_server, Task,local, self.feedback , done, offloading_server,{}  \n",
        "\n",
        "\n",
        "    def calcul_reward(self, offloading_server, transmission, computing_server, local_CPU, feedback, done ):\n",
        "      #if done:\n",
        "      \n",
        "      if local_CPU :\n",
        "        flag = False\n",
        "        reward= local_CPU\n",
        "        return -reward, flag # max((max_calcul, max_transmission, max_feedback ))\n",
        "      else :\n",
        "        flag = True \n",
        "        reward = transmission[offloading_server] + computing_server[offloading_server] + feedback[offloading_server]\n",
        "        return -reward, flag\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEUz2KGArfF1",
        "outputId": "28648bca-a73d-4509-cd0d-a44df6aa8c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "reward -7.143546609392465\n",
            "reward -7.319630609475766\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.00913784  9.50371808  3.45067585 49.4705092  17.82607925]\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308]Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.00913784  9.50371808  3.45067585 49.4705092  17.82607925]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.0091,  9.5037,\n",
            "          3.4507, 49.4705, 17.8261]])\n",
            "action 4 w00\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.00913784  9.50371808  3.45067585 49.4705092  17.82607925]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308]local_CPU 0\n",
            "\n",
            "reward -10.145180349146685\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.83008996  4.58003931 12.86612428 41.07982946 18.8641216 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.1131, 10.5631,\n",
            "         15.5251, 48.0127, 11.0752]])Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.83008996  4.58003931 12.86612428 41.07982946 18.8641216 ]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.8301,  4.5800,\n",
            "         12.8661, 41.0798, 18.8641]])\n",
            "local_CPU 0\n",
            "\n",
            "action 4 w00\n",
            "reward -9.998377293216873\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.83008996  4.58003931 12.86612428 41.07982946 18.8641216 ]\n",
            "\n",
            "Task 20.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]reward -12.748933876054796\n",
            "\n",
            "w00 episode  1924 reward -12.7\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.0153,  8.4526,\n",
            "         10.1913,  4.3910, 12.6926]])channels [1.830089963010757, 4.580039309171117, 12.86612427856277, 41.079829457370124, 18.86412160343557]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.09608816   5.36355862   3.02242729  37.48095871\n",
            "  23.68941914]\n",
            "\n",
            "action 4 w01\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.09608816   5.36355862   3.02242729  37.48095871\n",
            "  23.68941914]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.0961,\n",
            "           5.3636,   3.0224,  37.4810,  23.6894]])\n",
            "local_CPU 0\n",
            "\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.09608816   5.36355862   3.02242729  37.48095871\n",
            "  23.68941914]\n",
            "reward -12.762644344317108\n",
            "local_CPU 0\n",
            "reward -2.4794111888942187\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308]\n",
            "w01 episode  1925 reward -12.8\n",
            "channels [1.0153195922784441, 8.452617737938281, 10.191289765101525, 4.390956584043592, 12.692554031958359]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308][  1.           0.           1.           0.           1.\n",
            " 100.           3.13629355  10.51233522  11.00111168  21.45333964\n",
            "  18.08688234]\n",
            "Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.13629355  10.51233522  11.00111168  21.45333964\n",
            "  18.08688234]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.1131, 10.5631,\n",
            "         15.5251, 48.0127, 11.0752]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.1363,\n",
            "          10.5123,  11.0011,  21.4533,  18.0869]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.1130793  10.56314685 15.52508926 48.01274741 11.07517308]\n",
            "local_CPU 0\n",
            "\n",
            "reward -5.993292572760142\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.13629355  10.51233522  11.00111168  21.45333964\n",
            "  18.08688234]\n",
            "local_CPU 0\n",
            "reward -2.5111796625699903\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]Task 60.0\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.0153,  8.4526,\n",
            "         10.1913,  4.3910, 12.6926]])\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.5366,  2.2237,\n",
            "          8.9770, 58.0442, 14.0725]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.01531959  8.45261774 10.19128977  4.39095658 12.69255403]\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -9.270069159697039\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.13629355 10.51233522 11.00111168 21.45333964 18.08688234]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.13629355 10.51233522 11.00111168 21.45333964 18.08688234]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.1363, 10.5123,\n",
            "         11.0011, 21.4533, 18.0869]])\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -4.992951226879768\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.20221803  8.61534336  6.54479689 29.48649476 44.28606358]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.20221803  8.61534336  6.54479689 29.48649476 44.28606358]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.13629355 10.51233522 11.00111168 21.45333964 18.08688234]\n",
            "local_CPU 0\n",
            "\n",
            "reward -12.058774367096259\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]\n",
            "Task 20.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.2022,  8.6153,\n",
            "          6.5448, 29.4865, 44.2861]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.5366,  2.2237,\n",
            "          8.9770, 58.0442, 14.0725]])\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.20221803  8.61534336  6.54479689 29.48649476 44.28606358]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.53663451  2.2237445   8.97703094 58.04422152 14.07251235]\n",
            "local_CPU 0\n",
            "\n",
            "reward -14.993786090869943\n",
            "w00 episode  1926 reward -15.0\n",
            "local_CPU 0\n",
            "channels [1.5366345126760188, 2.223744499684598, 8.977030935943038, 58.04422151733951, 14.072512353622555]\n",
            "reward -7.197670951175839\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945][  1.           0.           1.           0.           1.\n",
            " 100.           2.20221803   8.61534336   6.54479689  29.48649476\n",
            "  44.28606358]\n",
            "\n",
            "Task 100.0\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.20221803   8.61534336   6.54479689  29.48649476\n",
            "  44.28606358]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.2022,\n",
            "           8.6153,   6.5448,  29.4865,  44.2861]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.5243,  2.2609,\n",
            "          2.0875, 48.0473, 46.0293]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.20221803   8.61534336   6.54479689  29.48649476\n",
            "  44.28606358]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.271327373191362\n",
            "reward -9.295609839544351\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623][ 1.          0.          1.          0.          1.         80.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945]\n",
            "\n",
            "Task 20.0\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.5243,  2.2609,\n",
            "          2.0875, 48.0473, 46.0293]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.5435, 14.0297,\n",
            "         12.9163, 75.5482, 70.5496]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.52429117  2.2609482   2.08751519 48.04730434 46.02928945]local_CPU 0\n",
            "reward -11.321637692656289\n",
            "w01 episode  1927 reward -11.3\n",
            "\n",
            "local_CPU 0\n",
            "channels [1.5434635414459839, 14.029750099602087, 12.916302700471276, 75.54819110661022, 70.54963622528842]\n",
            "reward -4.450499720062169\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.499246    12.74724539   5.80622007   8.44895984\n",
            "  15.2539709 ]Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.5435, 14.0297,\n",
            "         12.9163, 75.5482, 70.5496]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.499246    12.74724539   5.80622007   8.44895984\n",
            "  15.2539709 ]\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.54346354 14.0297501  12.9163027  75.54819111 70.54963623]\n",
            "local_CPU 0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.4992,\n",
            "          12.7472,   5.8062,   8.4490,  15.2540]])\n",
            "reward -6.526148754666879\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.499246   12.74724539  5.80622007  8.44895984 15.2539709 ]observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.499246    12.74724539   5.80622007   8.44895984\n",
            "  15.2539709 ]\n",
            "\n",
            "Task 40.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.499246   12.74724539  5.80622007  8.44895984 15.2539709 ]\n",
            "reward -2.492910740336358\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.4992, 12.7472,\n",
            "          5.8062,  8.4490, 15.2540]])[ 1.          0.          1.          0.          1.         80.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346]\n",
            "\n",
            "action 4 w00\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.499246   12.74724539  5.80622007  8.44895984 15.2539709 ]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.3434,  6.1522,\n",
            "          8.7551, 60.5907, 35.7597]])reward -9.406371629095963\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346]Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.3434,  6.1522,\n",
            "          8.7551, 60.5907, 35.7597]])\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "reward -4.615740380617023\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.34341132  6.15218351  8.75509524 60.59071108 35.75971346][ 1.          0.          1.          0.          1.         60.\n",
            "  3.64468818  9.60725586  2.70588769 20.57184409 63.02040775]\n",
            "\n",
            "local_CPU 0\n",
            "Task 60.0\n",
            "reward -11.649290646302807\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.64468818  9.60725586  2.70588769 20.57184409 63.02040775]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.6447,  9.6073,\n",
            "          2.7059, 20.5718, 63.0204]])w00 episode  1928 reward -11.6\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.64468818  9.60725586  2.70588769 20.57184409 63.02040775]channels [2.3434113172032576, 6.152183513559747, 8.755095237611116, 60.590711077107215, 35.75971345665907]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.64468818   9.60725586   2.70588769  20.57184409\n",
            "  63.02040775]\n",
            "local_CPU 0\n",
            "reward -6.661865664367747\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.64468818   9.60725586   2.70588769  20.57184409\n",
            "  63.02040775]\n",
            "Task 40.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.6447,\n",
            "           9.6073,   2.7059,  20.5718,  63.0204]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.6928, 11.4654,\n",
            "          6.9490, 52.4167,  8.5380]])\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]action 4 w00\n",
            "\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.64468818   9.60725586   2.70588769  20.57184409\n",
            "  63.02040775]\n",
            "reward -10.034844419209058\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]local_CPU 0\n",
            "\n",
            "Task 20.0\n",
            "reward -2.101522672960726\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.0458,  8.0888,\n",
            "          5.2848, 27.6531, 19.4238]])\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.6928, 11.4654,\n",
            "          6.9490, 52.4167,  8.5380]])\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.6927943  11.46536905  6.94900983 52.41672181  8.5380222 ]\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]reward -4.3231022280339655\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.0458,  8.0888,\n",
            "          5.2848, 27.6531, 19.4238]])local_CPU 0\n",
            "\n",
            "reward -12.352863296746746\n",
            "action 4 w00\n",
            "w01 episode  1929 reward -12.4\n",
            "channels [2.04576913963973, 8.088799466195985, 5.2848319620880195, 27.653059227697597, 19.423768421818252]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.52363295   6.91750176   9.13773499  25.20913243\n",
            "  44.9109079 ]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.04576914  8.08879947  5.28483196 27.65305923 19.42376842]\n",
            "\n",
            "reward -6.914037917294088\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.52363295  6.91750176  9.13773499 25.20913243 44.9109079 ]\n",
            "Task 40.0\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.52363295  6.91750176  9.13773499 25.20913243 44.9109079 ]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.52363295   6.91750176   9.13773499  25.20913243\n",
            "  44.9109079 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.5236,  6.9175,\n",
            "          9.1377, 25.2091, 44.9109]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.5236,\n",
            "           6.9175,   9.1377,  25.2091,  44.9109]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.52363295   6.91750176   9.13773499  25.20913243\n",
            "  44.9109079 ]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.52363295  6.91750176  9.13773499 25.20913243 44.9109079 ]\n",
            "local_CPU 0\n",
            "\n",
            "local_CPU 0\n",
            "reward -9.105664628496703\n",
            "reward -2.1096738333174474\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561][ 1.          0.          1.          0.          1.         20.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561]\n",
            "\n",
            "Task 80.0\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.0454,  1.4951,\n",
            "          8.0956, 37.2514, 62.8250]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.0454,  1.4951,\n",
            "          8.0956, 37.2514, 62.8250]])local_CPU 0\n",
            "reward -4.14497649300881\n",
            "\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.29734624  4.44626102  4.82397866 43.15896512 29.36143148]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.04542926  1.49509064  8.09561236 37.25143566 62.82501561]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "reward -11.19968905278117\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.29734624  4.44626102  4.82397866 43.15896512 29.36143148]\n",
            "w00 episode  1930 reward -11.2\n",
            "channels [1.0454292640815042, 1.4950906397744466, 8.09561235817185, 37.25143565791634, 62.825015610080726]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.2973,  4.4463,\n",
            "          4.8240, 43.1590, 29.3614]])\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.29734624   4.44626102   4.82397866  43.15896512\n",
            "  29.36143148]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.29734624  4.44626102  4.82397866 43.15896512 29.36143148]\n",
            "\n",
            "local_CPU 0\n",
            "reward -6.297686542913969\n",
            "Task 100.0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.29734624   4.44626102   4.82397866  43.15896512\n",
            "  29.36143148]Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.2973,\n",
            "           4.4463,   4.8240,  43.1590,  29.3614]])\n",
            "action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.29734624   4.44626102   4.82397866  43.15896512\n",
            "  29.36143148]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.3277,  3.6464,\n",
            "          7.8081, 27.3331, 18.2013]])\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815]reward -2.312746579732433\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815]\n",
            "local_CPU 0\n",
            "Task 80.0\n",
            "reward -8.763004983774607\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815][ 1.          0.          1.          0.          1.         20.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.3277,  3.6464,\n",
            "          7.8081, 27.3331, 18.2013]])Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.32765684  3.64642588  7.80806243 27.3331403  18.20133815]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.9070,  6.9826,\n",
            "         12.8511, 33.5249, 23.2075]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -5.0581857665255985\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415][ 1.          0.          1.          0.          1.         60.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415]\n",
            "\n",
            "local_CPU 0\n",
            "Task 60.0\n",
            "reward -11.191001286090664\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415]\n",
            "w01 episode  1931 reward -11.2\n",
            "channels [3.906969238834184, 6.982566948457257, 12.851145236811274, 33.524854436102416, 23.20754414658791]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.9070,  6.9826,\n",
            "         12.8511, 33.5249, 23.2075]])[  1.           0.           1.           0.           1.\n",
            " 100.           1.61087017   8.05694348   4.96173641  33.52113064\n",
            "  19.23410444]\n",
            "\n",
            "action 4 w00\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.90696924  6.98256695 12.85114524 33.52485444 23.20754415]observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.61087017   8.05694348   4.96173641  33.52113064\n",
            "  19.23410444]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.6109,\n",
            "           8.0569,   4.9617,  33.5211,  19.2341]])\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -7.665677276004833\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.61087017   8.05694348   4.96173641  33.52113064\n",
            "  19.23410444][ 1.          0.          1.          0.          1.         40.\n",
            "  1.61087017  8.05694348  4.96173641 33.52113064 19.23410444]\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.366708867327315\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.61087017  8.05694348  4.96173641 33.52113064 19.23410444][ 1.          0.          1.          0.          1.         80.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.6109,  8.0569,\n",
            "          4.9617, 33.5211, 19.2341]])Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962]\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.9952,  2.8286,\n",
            "          2.3401, 59.5188, 62.9844]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.61087017  8.05694348  4.96173641 33.52113064 19.23410444]\n",
            "action 4 w01\n",
            "\n",
            "local_CPU 0\n",
            "reward -10.300040257403516\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962][ 1.          0.          1.          0.          1.         20.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962]\n",
            "Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962]\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.9952,  2.8286,\n",
            "          2.3401, 59.5188, 62.9844]])\n",
            "action 4 w00\n",
            "reward -4.393955938057798\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.88024557 12.23598603  8.41683048 27.13837954 28.94275189]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.99522547  2.82857681  2.34014984 59.51875501 62.98436962]\n",
            "local_CPU 0\n",
            "reward -12.388170206433129\n",
            "\n",
            "Task 60.0\n",
            "w00 episode  1932 reward -12.4\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.88024557 12.23598603  8.41683048 27.13837954 28.94275189]channels [1.99522547054547, 2.828576806749331, 2.3401498386937187, 59.51875501463108, 62.98436961721506]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.8802, 12.2360,\n",
            "          8.4168, 27.1384, 28.9428]])[  1.           0.           1.           0.           1.\n",
            " 100.           0.88024557  12.23598603   8.41683048  27.13837954\n",
            "  28.94275189]\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.88024557 12.23598603  8.41683048 27.13837954 28.94275189]Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.88024557  12.23598603   8.41683048  27.13837954\n",
            "  28.94275189]\n",
            "local_CPU 0\n",
            "\n",
            "reward -6.566698786471683\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.8802,\n",
            "          12.2360,   8.4168,  27.1384,  28.9428]])\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]\n",
            "Task 40.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.88024557  12.23598603   8.41683048  27.13837954\n",
            "  28.94275189]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.8010, 12.7851,\n",
            "          9.7485, 29.1235, 12.1567]])local_CPU 0\n",
            "reward -2.3307170707111737\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]\n",
            "\n",
            "Task 80.0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]\n",
            "\n",
            "local_CPU 0\n",
            "reward -9.712598747239275\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.8010, 12.7851,\n",
            "          9.7485, 29.1235, 12.1567]])\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.80100805 12.7850911   9.74848301 29.1234593  12.15668946]\n",
            "\n",
            "local_CPU 0\n",
            "Task 20.0\n",
            "reward -6.009921874787594\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.0866,  7.9350,\n",
            "          5.8338, 16.6625, 37.0537]])\n",
            "Task 60.0\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.0866,  7.9350,\n",
            "          5.8338, 16.6625, 37.0537]])local_CPU 0\n",
            "reward -11.940559713000788\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.08662312  7.93504747  5.83384417 16.66245931 37.05372667]\n",
            "local_CPU 0\n",
            "reward -8.328025996097535\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.30135239  8.1392826   7.22640078 34.22233053 42.79948081]\n",
            "w01 episode  1933 reward -11.9\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.30135239  8.1392826   7.22640078 34.22233053 42.79948081]\n",
            "channels [2.0866231155080546, 7.935047466803939, 5.833844165695742, 16.662459314707046, 37.05372666527921]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           4.30135239   8.1392826    7.22640078  34.22233053\n",
            "  42.79948081]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.3014,  8.1393,\n",
            "          7.2264, 34.2223, 42.7995]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.30135239  8.1392826   7.22640078 34.22233053 42.79948081]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "reward -10.518206800300712\n",
            "[  1.           0.           1.           0.           1.\n",
            "  20.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.30135239   8.1392826    7.22640078  34.22233053\n",
            "  42.79948081]\n",
            "\n",
            "Task 20.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.3014,\n",
            "           8.1393,   7.2264,  34.2223,  42.7995]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            "  20.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]action 4 w01\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000,  20.0000,   1.0029,\n",
            "           7.6836,   6.1796,  33.0914, 118.5908]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.30135239   8.1392826    7.22640078  34.22233053\n",
            "  42.79948081]\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            "  20.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -12.483297744562039\n",
            "reward -2.097003369155139\n",
            "[  1.           0.           1.           0.           1.\n",
            "  80.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]w00 episode  1934 reward -12.5\n",
            "\n",
            "channels [1.002938165923104, 7.6835522973278065, 6.179635431960739, 33.09138910313234, 118.59076247379811]\n",
            "Task 80.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.0420012    9.67351534   3.88211985  28.03637289\n",
            "  46.91106069]observation [  1.           0.           1.           0.           1.\n",
            "  80.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000,  80.0000,   1.0029,\n",
            "           7.6836,   6.1796,  33.0914, 118.5908]])Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.0420012    9.67351534   3.88211985  28.03637289\n",
            "  46.91106069]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.0420,\n",
            "           9.6735,   3.8821,  28.0364,  46.9111]])\n",
            "\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            "  80.           1.00293817   7.6835523    6.17963543  33.0913891\n",
            " 118.59076247]\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.0420012    9.67351534   3.88211985  28.03637289\n",
            "  46.91106069]reward -4.01841301784347\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.0420012   9.67351534  3.88211985 28.03637289 46.91106069]local_CPU 0\n",
            "\n",
            "reward -2.214234616669504\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.0420012   9.67351534  3.88211985 28.03637289 46.91106069][ 1.          0.          1.          0.          1.         80.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]\n",
            "Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.0420,  9.6735,\n",
            "          3.8821, 28.0364, 46.9111]])\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.5738,  5.6358,\n",
            "          3.1036, 14.4455, 24.1920]])\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.0420012   9.67351534  3.88211985 28.03637289 46.91106069]action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]\n",
            "\n",
            "local_CPU 0\n",
            "reward -6.164595196188493\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]local_CPU 0\n",
            "reward -4.753865812586208\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.5738,  5.6358,\n",
            "          3.1036, 14.4455, 24.1920]])[ 1.          0.          1.          0.          1.         60.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]\n",
            "\n",
            "action 4 w01\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.57376408  5.63582189  3.10361522 14.44547231 24.19196266]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.5600, 10.1328,\n",
            "          3.5119, 39.3834, 47.5164]])local_CPU 0\n",
            "\n",
            "reward -8.530914832842264\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]\n",
            "Task 20.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.5600, 10.1328,\n",
            "          3.5119, 39.3834, 47.5164]])reward -6.920577133682956\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.78404583 12.20706397  9.11955419 10.66628871 44.26156843]\n",
            "\n",
            "action 4 w01\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.56002972 10.13284859  3.51189509 39.38338607 47.51640796]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.78404583 12.20706397  9.11955419 10.66628871 44.26156843]\n",
            "\n",
            "local_CPU 0\n",
            "reward -10.61827799019132\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.7840, 12.2071,\n",
            "          9.1196, 10.6663, 44.2616]])w01 episode  1935 reward -10.6\n",
            "channels [4.560029721282933, 10.132848585996978, 3.511895088010464, 39.38338606995108, 47.5164079627783]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           0.78404583  12.20706397   9.11955419  10.66628871\n",
            "  44.26156843]\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.78404583 12.20706397  9.11955419 10.66628871 44.26156843]\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.78404583  12.20706397   9.11955419  10.66628871\n",
            "  44.26156843]reward -9.165728283051248\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]\n",
            "Task 20.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.7840,\n",
            "          12.2071,   9.1196,  10.6663,  44.2616]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.1159,  2.6791,\n",
            "          5.6209, 24.0266, 24.2123]])\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.78404583  12.20706397   9.11955419  10.66628871\n",
            "  44.26156843]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]\n",
            "local_CPU 0\n",
            "reward -2.1740362903037815\n",
            "local_CPU 0\n",
            "reward -11.713461650325613\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]\n",
            "Task 80.0\n",
            "w00 episode  1936 reward -11.7\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]channels [1.1158847962357075, 2.6790943661010043, 5.620859502612828, 24.026622564211387, 24.21232483322314]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.2927644    5.81853722   4.97378219  33.86940971\n",
            "  72.19086348]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.1159,  2.6791,\n",
            "          5.6209, 24.0266, 24.2123]])\n",
            "Task 100.0\n",
            "\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.2927644    5.81853722   4.97378219  33.86940971\n",
            "  72.19086348]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.1158848   2.67909437  5.6208595  24.02662256 24.21232483]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.2928,\n",
            "           5.8185,   4.9738,  33.8694,  72.1909]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -4.550080106220905\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.2927644    5.81853722   4.97378219  33.86940971\n",
            "  72.19086348]\n",
            "local_CPU 0\n",
            "reward -2.071063476204573\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.2927644   5.81853722  4.97378219 33.86940971 72.19086348]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.2927644   5.81853722  4.97378219 33.86940971 72.19086348]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.2704,  2.8904,\n",
            "          5.7972, 30.7695, 25.8899]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.2928,  5.8185,\n",
            "          4.9738, 33.8694, 72.1909]])\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "local_CPU 0\n",
            "reward -4.455347598881634\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.2927644   5.81853722  4.97378219 33.86940971 72.19086348]\n",
            "\n",
            "local_CPU 0\n",
            "reward -6.572729438155345\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.2704,  2.8904,\n",
            "          5.7972, 30.7695, 25.8899]])Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.2759,  7.5905,\n",
            "         11.4382, 52.7418, 34.6288]])action 4 w01\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.27044331  2.89043967  5.79721358 30.76947111 25.88994603]\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -8.772226348667898\n",
            "reward -6.696436243966999\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079][ 1.          0.          1.          0.          1.         40.\n",
            "  2.02905802 14.24939429  5.43938413 14.39469315 30.30411754]\n",
            "\n",
            "Task 40.0\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.02905802 14.24939429  5.43938413 14.39469315 30.30411754]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.0291, 14.2494,\n",
            "          5.4394, 14.3947, 30.3041]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.2759,  7.5905,\n",
            "         11.4382, 52.7418, 34.6288]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.27592508  7.59050829 11.43820418 52.7418024  34.62881079]\n",
            "local_CPU 0\n",
            "reward -10.881498060283631\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.02905802 14.24939429  5.43938413 14.39469315 30.30411754]\n",
            "local_CPU 0\n",
            "reward -9.00267645271872\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]\n",
            "w01 episode  1937 reward -10.9\n",
            "channels [2.275925075826017, 7.590508292726078, 11.43820418349763, 52.7418023969003, 34.628810786535176]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.1187,  8.0502,\n",
            "          8.2547, 36.6653,  8.4555]])[  1.           0.           1.           0.           1.\n",
            " 100.           2.02905802  14.24939429   5.43938413  14.39469315\n",
            "  30.30411754]\n",
            "\n",
            "Task 100.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]\n",
            "local_CPU 0\n",
            "reward -11.404050610096432\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.02905802  14.24939429   5.43938413  14.39469315\n",
            "  30.30411754]\n",
            "w00 episode  1938 reward -11.4\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.0291,\n",
            "          14.2494,   5.4394,  14.3947,  30.3041]])\n",
            "channels [3.118682802884547, 8.050156785959018, 8.2546922739743, 36.665253826220095, 8.455470962651656]\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.02905802  14.24939429   5.43938413  14.39469315\n",
            "  30.30411754][  1.           0.           1.           0.           1.\n",
            " 100.           3.41365943  10.22274586   7.47333741  22.3812712\n",
            "  26.02708289]\n",
            "Task 100.0\n",
            "\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.41365943  10.22274586   7.47333741  22.3812712\n",
            "  26.02708289]reward -2.1560997818155814\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.4137,\n",
            "          10.2227,   7.4733,  22.3813,  26.0271]])\n",
            "\n",
            "Task 80.0\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.41365943  10.22274586   7.47333741  22.3812712\n",
            "  26.02708289]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.1187,  8.0502,\n",
            "          8.2547, 36.6653,  8.4555]])\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -2.3753728852501057\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.1186828   8.05015679  8.25469227 36.66525383  8.45547096]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]local_CPU 0\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.9340,  8.6060,\n",
            "         11.0491, 38.0705, 11.0741]])reward -5.47806338811033\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.41365943 10.22274586  7.47333741 22.3812712  26.02708289]\n",
            "Task 60.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.41365943 10.22274586  7.47333741 22.3812712  26.02708289]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.4137, 10.2227,\n",
            "          7.4733, 22.3813, 26.0271]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.41365943 10.22274586  7.47333741 22.3812712  26.02708289]reward -5.731832092554503\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]\n",
            "\n",
            "local_CPU 0\n",
            "reward -7.667419815654364\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.8208,  5.5108,\n",
            "          3.8461, 55.7657, 11.5295]])Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.9340,  8.6060,\n",
            "         11.0491, 38.0705, 11.0741]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.93399326  8.60596038 11.04909208 38.07048948 11.07410935]\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -10.34632331521409\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]reward -9.063061608514673\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.8208,  5.5108,\n",
            "          3.8461, 55.7657, 11.5295]])[ 1.          0.          1.          0.          1.         40.\n",
            "  0.46041003  7.42834558  8.19288073 40.08060681 16.971117  ]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.82075879  5.51082397  3.84605629 55.76572833 11.52952861]\n",
            "local_CPU 0\n",
            "reward -13.053948416817851\n",
            "w01 episode  1939 reward -13.1\n",
            "\n",
            "channels [3.820758793273086, 5.510823966865909, 3.84605628837558, 55.765728332749646, 11.529528612082075]\n",
            "Task 40.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           0.46041003   7.42834558   8.19288073  40.08060681\n",
            "  16.971117  ]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.46041003  7.42834558  8.19288073 40.08060681 16.971117  ]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.4604,  7.4283,\n",
            "          8.1929, 40.0806, 16.9711]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.46041003   7.42834558   8.19288073  40.08060681\n",
            "  16.971117  ]\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.46041003  7.42834558  8.19288073 40.08060681 16.971117  ]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.4604,\n",
            "           7.4283,   8.1929,  40.0806,  16.9711]])\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.768956897641548\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.46041003   7.42834558   8.19288073  40.08060681\n",
            "  16.971117  ]\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.3679088032007773\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.6762, 12.2740,\n",
            "          4.9425, 17.5904, 35.2827]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.6762, 12.2740,\n",
            "          4.9425, 17.5904, 35.2827]])\n",
            "action 4 w01\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]\n",
            "local_CPU 0\n",
            "reward -14.074664364637803\n",
            "w00 episode  1940 reward -14.1\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.67615422 12.2739755   4.94248131 17.59035648 35.28269402]\n",
            "channels [1.676154219273013, 12.273975503616635, 4.942481311875546, 17.590356482802044, 35.28269402324644]\n",
            "local_CPU 0\n",
            "reward -4.570068606336202\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.10104624  7.50347613  5.00964348 36.47883211 24.03516609]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.10104624  7.50347613  5.00964348 36.47883211 24.03516609]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.1010,  7.5035,\n",
            "          5.0096, 36.4788, 24.0352]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.10104624  7.50347613  5.00964348 36.47883211 24.03516609]\n",
            "local_CPU 0\n",
            "reward -6.909768618404879\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.4203,  5.7123,\n",
            "          7.2603, 54.4797, 67.8001]])\n",
            "action 4 w01\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.10104624   7.50347613   5.00964348  36.47883211\n",
            "  24.03516609]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "reward -8.94705027538588\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.10104624   7.50347613   5.00964348  36.47883211\n",
            "  24.03516609]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.1010,\n",
            "           7.5035,   5.0096,  36.4788,  24.0352]])[ 1.          0.          1.          0.          1.         20.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344]\n",
            "\n",
            "Task 20.0\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.10104624   7.50347613   5.00964348  36.47883211\n",
            "  24.03516609]\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344]reward -2.5192076301473016\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.3835,  8.6692,\n",
            "         11.1093, 43.9700, 52.8297]])[ 1.          0.          1.          0.          1.         80.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344]\n",
            "local_CPU 0\n",
            "Task 80.0\n",
            "reward -10.970995104121128\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.4203,  5.7123,\n",
            "          7.2603, 54.4797, 67.8001]])\n",
            "w01 episode  1941 reward -11.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.42029099  5.71230825  7.26031724 54.47974327 67.80008198]\n",
            "local_CPU 0\n",
            "reward -4.606897226996814\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344]channels [0.3835285732563012, 8.66917102447446, 11.109285796727317, 43.96999742909749, 52.829723439236254]\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344][  1.           0.           1.           0.           1.\n",
            " 100.           3.84441225   5.81903205  11.0202227   35.66660841\n",
            "  28.92880758]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.3835,  8.6692,\n",
            "         11.1093, 43.9700, 52.8297]])\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.84441225   5.81903205  11.0202227   35.66660841\n",
            "  28.92880758]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.8444,\n",
            "           5.8190,  11.0202,  35.6666,  28.9288]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.38352857  8.66917102 11.1092858  43.96999743 52.82972344]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.84441225   5.81903205  11.0202227   35.66660841\n",
            "  28.92880758]\n",
            "local_CPU 0\n",
            "\n",
            "reward -6.715739641699069\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.84441225  5.81903205 11.0202227  35.66660841 28.92880758]local_CPU 0\n",
            "reward -2.221312975466928\n",
            "\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.84441225  5.81903205 11.0202227  35.66660841 28.92880758]\n",
            "Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.8444,  5.8190,\n",
            "         11.0202, 35.6666, 28.9288]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.0188,  2.6271,\n",
            "          7.7739, 19.5340, 29.9285]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.84441225  5.81903205 11.0202227  35.66660841 28.92880758]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]local_CPU 0\n",
            "reward -9.082663906964193\n",
            "\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]reward -4.481367124319385\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  6.45991657 18.51692348  8.46750313 57.27448673 21.98462625]Task 20.0\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  6.45991657 18.51692348  8.46750313 57.27448673 21.98462625]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.0188,  2.6271,\n",
            "          7.7739, 19.5340, 29.9285]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.01879293  2.62710755  7.77394542 19.5339792  29.92852297]\n",
            "local_CPU 0\n",
            "reward -11.471898515666497\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  6.4599, 18.5169,\n",
            "          8.4675, 57.2745, 21.9846]])w00 episode  1942 reward -11.5\n",
            "\n",
            "channels [3.018792926750941, 2.627107554742971, 7.773945418253681, 19.53397920412627, 29.928522974792806]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  6.45991657 18.51692348  8.46750313 57.27448673 21.98462625][  1.           0.           1.           0.           1.\n",
            " 100.           6.45991657  18.51692348   8.46750313  57.27448673\n",
            "  21.98462625]\n",
            "local_CPU 0\n",
            "\n",
            "reward -6.840062131221318\n",
            "Task 100.0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]observation [  1.           0.           1.           0.           1.\n",
            " 100.           6.45991657  18.51692348   8.46750313  57.27448673\n",
            "  21.98462625]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   6.4599,\n",
            "          18.5169,   8.4675,  57.2745,  21.9846]])\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           6.45991657  18.51692348   8.46750313  57.27448673\n",
            "  21.98462625]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.4454, 12.8607,\n",
            "          9.0084, 32.7400, 17.8141]])\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.5679355074934973\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]local_CPU 0\n",
            "\n",
            "reward -9.182795139515376\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.4454, 12.8607,\n",
            "          9.0084, 32.7400, 17.8141]])[ 1.          0.          1.          0.          1.         20.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]\n",
            "\n",
            "action 4 w00\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.44539851 12.86074806  9.00835718 32.74000564 17.81413824]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.4995,  6.1934,\n",
            "          6.4553, 21.2487, 51.2865]])reward -5.224921972658572\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]action 4 w01\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.219926401245779\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.4995,  6.1934,\n",
            "          6.4553, 21.2487, 51.2865]])w01 episode  1943 reward -11.2\n",
            "channels [2.4994829224833475, 6.193364163374532, 6.455260474845043, 21.248736605004453, 51.28647973053973]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.02230449  14.04974043  10.90103497  27.79163274\n",
            "  23.04232956]\n",
            "action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.49948292  6.19336416  6.45526047 21.24873661 51.28647973]Task 100.0\n",
            "\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.02230449  14.04974043  10.90103497  27.79163274\n",
            "  23.04232956]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.0223,\n",
            "          14.0497,  10.9010,  27.7916,  23.0423]])reward -7.346240914992447\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.02230449 14.04974043 10.90103497 27.79163274 23.04232956]action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.02230449  14.04974043  10.90103497  27.79163274\n",
            "  23.04232956]\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.2885931774463697\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.02230449 14.04974043 10.90103497 27.79163274 23.04232956]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.0223, 14.0497,\n",
            "         10.9010, 27.7916, 23.0423]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.2928,  4.9822,\n",
            "          7.8154, 12.7894, 34.1248]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.02230449 14.04974043 10.90103497 27.79163274 23.04232956]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735]local_CPU 0\n",
            "reward -9.839064917868534\n",
            "\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735]\n",
            "reward -4.483852902474947\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.3751618   1.24737955  1.77786898  8.2482001   7.97725059]Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735]\n",
            "Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.3751618   1.24737955  1.77786898  8.2482001   7.97725059]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.3752,  1.2474,\n",
            "          1.7779,  8.2482,  7.9773]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.2928,  4.9822,\n",
            "          7.8154, 12.7894, 34.1248]])action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.3751618   1.24737955  1.77786898  8.2482001   7.97725059]\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "reward -7.584208329742026\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.29283262  4.98215939  7.81537308 12.78935978 34.12476735][ 1.          0.          1.          0.          1.         40.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]\n",
            "\n",
            "Task 40.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]reward -12.14601910860378\n",
            "\n",
            "w00 episode  1944 reward -12.1\n",
            "channels [1.2928326238705918, 4.982159388826292, 7.8153730817400255, 12.789359782742855, 34.124767345604084]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.3751618    1.24737955   1.77786898   8.2482001\n",
            "   7.97725059]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.3751618    1.24737955   1.77786898   8.2482001\n",
            "   7.97725059]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.3752,\n",
            "           1.2474,   1.7779,   8.2482,   7.9773]])\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.3751618    1.24737955   1.77786898   8.2482001\n",
            "   7.97725059]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.8071,  8.5537,\n",
            "          8.8270, 78.0543, 48.3341]])local_CPU 0\n",
            "\n",
            "action 4 w01\n",
            "reward -2.4146127815112104\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]\n",
            "local_CPU 0\n",
            "reward -9.663976688123734\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.8071,  8.5537,\n",
            "          8.8270, 78.0543, 48.3341]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.80710365  8.55369651  8.82700441 78.0543041  48.3341125 ]\n",
            "local_CPU 0\n",
            "reward -4.573357269715984\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.3800,  5.4403,\n",
            "          2.0848, 26.7232, 48.8934]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.3800,  5.4403,\n",
            "          2.0848, 26.7232, 48.8934]])\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.38000857  5.44025213  2.08476935 26.72321178 48.89343732]reward -6.717762134272476\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.38350961  9.98824756  7.66381573 35.0841798  51.92218957]\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.38350961  9.98824756  7.66381573 35.0841798  51.92218957]\n",
            "local_CPU 0\n",
            "reward -11.725574728784022\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.3835,  9.9882,\n",
            "          7.6638, 35.0842, 51.9222]])\n",
            "action 4 w00\n",
            "w01 episode  1945 reward -11.7\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.38350961  9.98824756  7.66381573 35.0841798  51.92218957]\n",
            "local_CPU 0\n",
            "channels [4.380008570986101, 5.440252130667476, 2.0847693491276527, 26.723211777840696, 48.89343731737476]\n",
            "reward -8.84417536240191\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ][  1.           0.           1.           0.           1.\n",
            " 100.           2.38350961   9.98824756   7.66381573  35.0841798\n",
            "  51.92218957]\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.2589, 10.2269,\n",
            "          0.1656, 31.4068, 27.2003]])\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.38350961   9.98824756   7.66381573  35.0841798\n",
            "  51.92218957]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.3835,\n",
            "           9.9882,   7.6638,  35.0842,  51.9222]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -11.329558476451169\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.38350961   9.98824756   7.66381573  35.0841798\n",
            "  51.92218957]w00 episode  1946 reward -11.3\n",
            "channels [0.25894164956920446, 10.226926253737595, 0.16557306263565794, 31.40684346912489, 27.200333998491978]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.17219858   5.38255768  10.13853341  27.33853902\n",
            "  69.05066375]local_CPU 0\n",
            "\n",
            "reward -2.047831304751168\n",
            "Task 100.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.17219858   5.38255768  10.13853341  27.33853902\n",
            "  69.05066375]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.1722,\n",
            "           5.3826,  10.1385,  27.3385,  69.0507]])\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ]\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.17219858   5.38255768  10.13853341  27.33853902\n",
            "  69.05066375]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.2589, 10.2269,\n",
            "          0.1656, 31.4068, 27.2003]])\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -2.0781482711574113\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.25894165 10.22692625  0.16557306 31.40684347 27.200334  ]\n",
            "local_CPU 0\n",
            "reward -4.3921147479695035\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.17219858  5.38255768 10.13853341 27.33853902 69.05066375]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.17219858  5.38255768 10.13853341 27.33853902 69.05066375][ 1.          0.          1.          0.          1.         80.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.1722,  5.3826,\n",
            "         10.1385, 27.3385, 69.0507]])Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.2876,  4.6301,\n",
            "          8.0996, 20.6157, 25.8423]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.17219858  5.38255768 10.13853341 27.33853902 69.05066375]\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]\n",
            "reward -6.418777681592967\n",
            "local_CPU 0\n",
            "reward -4.468471886427784\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.2876,  4.6301,\n",
            "          8.0996, 20.6157, 25.8423]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.1630,  4.2923,\n",
            "          6.4035,  2.9549, 36.7108]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.28760844  4.63006331  8.09955372 20.61572941 25.84225794]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -8.62584615141044\n",
            "reward -6.746901346816557\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079][ 1.          0.          1.          0.          1.         40.\n",
            "  1.24720858 12.30168006  7.8994677  41.43556614  9.04263718]\n",
            "\n",
            "Task 20.0\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.24720858 12.30168006  7.8994677  41.43556614  9.04263718]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.1630,  4.2923,\n",
            "          6.4035,  2.9549, 36.7108]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.2472, 12.3017,\n",
            "          7.8995, 41.4356,  9.0426]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.16298146  4.29230808  6.40351928  2.95490317 36.71076079]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.24720858 12.30168006  7.8994677  41.43556614  9.04263718]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -10.803515324867131\n",
            "reward -9.120352225584162\n",
            "w01 episode  1947 reward -10.8\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]channels [1.1629814597614652, 4.2923080791850365, 6.403519279621962, 2.954903173016603, 36.71076079477504]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.24720858  12.30168006   7.8994677   41.43556614\n",
            "   9.04263718]Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.24720858  12.30168006   7.8994677   41.43556614\n",
            "   9.04263718]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.7004, 11.2474,\n",
            "         12.9352, 11.1483, 17.8425]])\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.2472,\n",
            "          12.3017,   7.8995,  41.4356,   9.0426]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "reward -11.866966653785036\n",
            "w00 episode  1948 reward -11.9\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.24720858  12.30168006   7.8994677   41.43556614\n",
            "   9.04263718]channels [2.700448355706427, 11.247417856581412, 12.935165950213356, 11.1483202574938, 17.84254737623382]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           0.98336058   8.67993306   3.66859889  43.03314426\n",
            "  19.37587699]local_CPU 0\n",
            "reward -3.4038073088518015\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]\n",
            "\n",
            "Task 100.0\n",
            "Task 80.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.98336058   8.67993306   3.66859889  43.03314426\n",
            "  19.37587699]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.9834,\n",
            "           8.6799,   3.6686,  43.0331,  19.3759]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.7004, 11.2474,\n",
            "         12.9352, 11.1483, 17.8425]])\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.98336058   8.67993306   3.66859889  43.03314426\n",
            "  19.37587699]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.70044836 11.24741786 12.93516595 11.14832026 17.84254738]\n",
            "local_CPU 0\n",
            "reward -5.857780584625995\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.98336058  8.67993306  3.66859889 43.03314426 19.37587699]\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.98336058  8.67993306  3.66859889 43.03314426 19.37587699]\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.9834,  8.6799,\n",
            "          3.6686, 43.0331, 19.3759]])\n",
            "reward -2.6511468455804925\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.98336058  8.67993306  3.66859889 43.03314426 19.37587699]\n",
            "Task 80.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]reward -8.249429814133881\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.6968,  8.6873,\n",
            "          4.6486, 35.9133, 12.8696]])\n",
            "\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.6968,  8.6873,\n",
            "          4.6486, 35.9133, 12.8696]])\n",
            "reward -5.723306395930063\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.69681352  8.68732831  4.64863718 35.91330827 12.86959815]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]local_CPU 0\n",
            "reward -10.796794234128255\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  5.7319,  9.4809,\n",
            "          4.1065, 12.3066, 14.7525]])[ 1.          0.          1.          0.          1.         20.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  5.7319,  9.4809,\n",
            "          4.1065, 12.3066, 14.7525]])reward -8.593804063125106\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.32867412  0.72865011  8.36317523 29.50321142 26.92325444]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  5.73188144  9.48087471  4.10651896 12.30663867 14.75252375]\n",
            "\n",
            "Task 40.0\n",
            "local_CPU 0\n",
            "reward -13.247105552602761\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.32867412  0.72865011  8.36317523 29.50321142 26.92325444]w01 episode  1949 reward -13.2\n",
            "\n",
            "channels [5.73188143913046, 9.480874708690283, 4.106518964740988, 12.306638671673651, 14.752523750740503]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.3287,  0.7287,\n",
            "          8.3632, 29.5032, 26.9233]])[  1.           0.           1.           0.           1.\n",
            " 100.           3.32867412   0.72865011   8.36317523  29.50321142\n",
            "  26.92325444]\n",
            "\n",
            "action 4 w00\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.32867412  0.72865011  8.36317523 29.50321142 26.92325444]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.32867412   0.72865011   8.36317523  29.50321142\n",
            "  26.92325444]\n",
            "local_CPU 0\n",
            "reward -11.029606551345257\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]\n",
            "Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.3287,\n",
            "           0.7287,   8.3632,  29.5032,  26.9233]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.9633,  8.0871,\n",
            "          4.9587, 37.3973, 16.0055]])\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.32867412   0.72865011   8.36317523  29.50321142\n",
            "  26.92325444]action 4 w00\n",
            "\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]reward -2.2822777223150603\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]local_CPU 0\n",
            "reward -13.799825819416627\n",
            "w00 episode  1950 reward -13.8\n",
            "channels [1.963276445809134, 8.087136458805979, 4.958653596222356, 37.397307673509644, 16.0055393211229]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.9633,  8.0871,\n",
            "          4.9587, 37.3973, 16.0055]])[  1.           0.           1.           0.           1.\n",
            " 100.           1.53516783   6.02273118   0.77058726  24.37102334\n",
            "  18.01688672]\n",
            "\n",
            "Task 100.0\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.53516783   6.02273118   0.77058726  24.37102334\n",
            "  18.01688672]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.96327645  8.08713646  4.9586536  37.39730767 16.00553932]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.5352,\n",
            "           6.0227,   0.7706,  24.3710,  18.0169]])\n",
            "local_CPU 0\n",
            "reward -4.682787455090009\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.53516783   6.02273118   0.77058726  24.37102334\n",
            "  18.01688672][ 1.          0.          1.          0.          1.         60.\n",
            "  1.53516783  6.02273118  0.77058726 24.37102334 18.01688672]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.53516783  6.02273118  0.77058726 24.37102334 18.01688672]reward -2.742401397394492\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.5352,  6.0227,\n",
            "          0.7706, 24.3710, 18.0169]])\n",
            "\n",
            "Task 80.0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.53516783  6.02273118  0.77058726 24.37102334 18.01688672]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.1618, 10.8120,\n",
            "          8.5097, 38.3237, 13.1583]])local_CPU 0\n",
            "\n",
            "reward -7.138119338748641\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]\n",
            "\n",
            "Task 40.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]reward -6.162318298091966\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.1618, 10.8120,\n",
            "          8.5097, 38.3237, 13.1583]])\n",
            "\n",
            "Task 60.0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.16183208 10.81197159  8.50965599 38.32365281 13.15829156]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.1670,  9.1644,\n",
            "          5.0722,  9.9131, 12.4188]])\n",
            "local_CPU 0\n",
            "reward -10.09329893832075\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]\n",
            "local_CPU 0\n",
            "\n",
            "reward -9.49726200043786\n",
            "Task 20.0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.56589034  5.9992556  11.74969598  4.93867725 18.96479753]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]\n",
            "\n",
            "Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.56589034  5.9992556  11.74969598  4.93867725 18.96479753]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.1670,  9.1644,\n",
            "          5.0722,  9.9131, 12.4188]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.5659,  5.9993,\n",
            "         11.7497,  4.9387, 18.9648]])action 4 w01\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.16700792  9.16438998  5.07215337  9.91312554 12.41876597]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.56589034  5.9992556  11.74969598  4.93867725 18.96479753]local_CPU 0\n",
            "\n",
            "local_CPU 0\n",
            "reward -12.897017938505554\n",
            "reward -12.09630076188335\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]\n",
            "Task 20.0\n",
            "w01 episode  1951 reward -12.9\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]channels [1.1670079173970325, 9.164389984997257, 5.072153367437445, 9.913125543293097, 12.418765968561946]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.1444,  1.6099,\n",
            "          2.5655, 57.9071, 24.1644]])\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.56589034   5.9992556   11.74969598   4.93867725\n",
            "  18.96479753]action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.56589034   5.9992556   11.74969598   4.93867725\n",
            "  18.96479753]reward -14.558224389336155\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.5659,\n",
            "           5.9993,  11.7497,   4.9387,  18.9648]])w00 episode  1952 reward -14.6\n",
            "\n",
            "action 4 w01\n",
            "channels [3.144372864532276, 1.609858065525967, 2.5655258287544713, 57.90706579402866, 24.16443651898591]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.56589034   5.9992556   11.74969598   4.93867725\n",
            "  18.96479753]\n",
            "local_CPU 0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           5.15365945   6.14229561   5.22301344  36.49515038\n",
            "  26.71206643]\n",
            "reward -2.3124239286465342\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.15365945   6.14229561   5.22301344  36.49515038\n",
            "  26.71206643]Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   5.1537,\n",
            "           6.1423,   5.2230,  36.4952,  26.7121]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.1444,  1.6099,\n",
            "          2.5655, 57.9071, 24.1644]])action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.15365945   6.14229561   5.22301344  36.49515038\n",
            "  26.71206643]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.14437286  1.60985807  2.56552583 57.90706579 24.16443652]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.3610297716622766\n",
            "reward -4.583897165501451\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554][ 1.          0.          1.          0.          1.         60.\n",
            "  5.15365945  6.14229561  5.22301344 36.49515038 26.71206643]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554]\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.4890,  9.3679,\n",
            "          7.2603, 64.9620, 18.7895]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  5.15365945  6.14229561  5.22301344 36.49515038 26.71206643]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  5.1537,  6.1423,\n",
            "          5.2230, 36.4952, 26.7121]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554]\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.9683211503141615\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]\n",
            "action 4 w01\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.7263,  7.3611,\n",
            "          3.4350, 26.1947,  3.1197]])\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  5.15365945  6.14229561  5.22301344 36.49515038 26.71206643]\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]\n",
            "local_CPU 0\n",
            "reward -7.885152801638447\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.12787278  3.88042966  7.32075295 40.92528312 33.4256723 ]\n",
            "local_CPU 0\n",
            "Task 40.0\n",
            "reward -6.764959770572191\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.12787278  3.88042966  7.32075295 40.92528312 33.4256723 ]\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.1279,  3.8804,\n",
            "          7.3208, 40.9253, 33.4257]])\n",
            "Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554]\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.4890,  9.3679,\n",
            "          7.2603, 64.9620, 18.7895]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.12787278  3.88042966  7.32075295 40.92528312 33.4256723 ]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.48899138  9.36792775  7.26030241 64.96197197 18.78945554]reward -10.178504279931646\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]local_CPU 0\n",
            "reward -9.081733973298759\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]Task 20.0\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.7263,  7.3611,\n",
            "          3.4350, 26.1947,  3.1197]])\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.72632072  7.36110392  3.43496804 26.1946812   3.11966457]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.9127,  6.4844,\n",
            "          6.5215, 20.5725, 32.5164]])\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]reward -11.708048448697726\n",
            "w01 episode  1953 reward -11.7\n",
            "\n",
            "channels [2.7263207201786708, 7.361103920943921, 3.434968042675454, 26.19468120454563, 3.1196645740153888]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.12787278   3.88042966   7.32075295  40.92528312\n",
            "  33.4256723 ]local_CPU 0\n",
            "reward -12.451199898530671\n",
            "\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.12787278   3.88042966   7.32075295  40.92528312\n",
            "  33.4256723 ]\n",
            "w00 episode  1954 reward -12.5\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.1279,\n",
            "           3.8804,   7.3208,  40.9253,  33.4257]])\n",
            "channels [2.9127225281061544, 6.484366828237784, 6.521489394799671, 20.572532079157977, 32.51636412170406]\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.12787278   3.88042966   7.32075295  40.92528312\n",
            "  33.4256723 ][  1.           0.           1.           0.           1.\n",
            " 100.           1.76658932   2.39001583   6.18141966  24.54386476\n",
            "  21.73397056]\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.171853341079928\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.76658932   2.39001583   6.18141966  24.54386476\n",
            "  21.73397056][ 1.          0.          1.          0.          1.         80.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.7666,\n",
            "           2.3900,   6.1814,  24.5439,  21.7340]])\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.9127,  6.4844,\n",
            "          6.5215, 20.5725, 32.5164]])action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.76658932   2.39001583   6.18141966  24.54386476\n",
            "  21.73397056]\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "reward -2.5396537086097966\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.91272253  6.48436683  6.52148939 20.57253208 32.51636412]\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]reward -4.305785593277415\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.76658932  2.39001583  6.18141966 24.54386476 21.73397056]Task 80.0\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.76658932  2.39001583  6.18141966 24.54386476 21.73397056]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.0433,  5.6283,\n",
            "          4.2523, 48.8500, 26.2636]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.7666,  2.3900,\n",
            "          6.1814, 24.5439, 21.7340]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.76658932  2.39001583  6.18141966 24.54386476 21.73397056]action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]\n",
            "local_CPU 0\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.928213417974959\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238]\n",
            "reward -6.624158507140808\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238][ 1.          0.          1.          0.          1.         40.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.7247,  6.6195,\n",
            "          4.7265, 41.1249, 16.0926]])\n",
            "\n",
            "action 4 w00\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.0433,  5.6283,\n",
            "          4.2523, 48.8500, 26.2636]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.04332676  5.62833666  4.25234813 48.8499578  26.26355936]\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238]\n",
            "local_CPU 0\n",
            "reward -8.027407304280594\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.21806052  1.921663    4.96166301 54.26694704 24.40353306]local_CPU 0\n",
            "\n",
            "reward -8.83611554833696\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238]\n",
            "Task 20.0\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.21806052  1.921663    4.96166301 54.26694704 24.40353306]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.2181,  1.9217,\n",
            "          4.9617, 54.2669, 24.4035]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.7247,  6.6195,\n",
            "          4.7265, 41.1249, 16.0926]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.21806052  1.921663    4.96166301 54.26694704 24.40353306]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.72472706  6.61954434  4.72653507 41.12486339 16.09263238]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -10.44057751993488\n",
            "reward -11.613481440137503\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]w01 episode  1955 reward -11.6\n",
            "\n",
            "channels [3.724727055561367, 6.6195443386478185, 4.726535072869665, 41.12486338757743, 16.092632376053565]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           4.21806052   1.921663     4.96166301  54.26694704\n",
            "  24.40353306]Task 20.0\n",
            "\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.21806052   1.921663     4.96166301  54.26694704\n",
            "  24.40353306]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.2815,  2.5655,\n",
            "          5.4688, 14.2796, 47.0500]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.2181,\n",
            "           1.9217,   4.9617,  54.2669,  24.4035]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.21806052   1.921663     4.96166301  54.26694704\n",
            "  24.40353306]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -12.604759930932229\n",
            "reward -2.21095002628465\n",
            "w00 episode  1956 reward -12.6\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]\n",
            "channels [1.2815105038759667, 2.5655018114664343, 5.4688430120814795, 14.279552714757516, 47.04998686003509]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.20460457  11.29521561   8.81458018  43.58840224\n",
            "  14.57211335]Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.2815,  2.5655,\n",
            "          5.4688, 14.2796, 47.0500]])Task 100.0\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.2815105   2.56550181  5.46884301 14.27955271 47.04998686]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.20460457  11.29521561   8.81458018  43.58840224\n",
            "  14.57211335]\n",
            "local_CPU 0\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.2046,\n",
            "          11.2952,   8.8146,  43.5884,  14.5721]])\n",
            "reward -4.292736373790698\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.20460457 11.29521561  8.81458018 43.58840224 14.57211335]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.20460457  11.29521561   8.81458018  43.58840224\n",
            "  14.57211335]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.20460457 11.29521561  8.81458018 43.58840224 14.57211335]\n",
            "reward -2.886833784766583\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.2046, 11.2952,\n",
            "          8.8146, 43.5884, 14.5721]])\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.20460457 11.29521561  8.81458018 43.58840224 14.57211335]\n",
            "\n",
            "Task 80.0\n",
            "local_CPU 0\n",
            "reward -6.7510542276053265\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563][ 1.          0.          1.          0.          1.         40.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.2629, 11.2319,\n",
            "          6.7308, 11.9554, 18.1985]])Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563]\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.2629, 11.2319,\n",
            "          6.7308, 11.9554, 18.1985]])local_CPU 0\n",
            "\n",
            "reward -5.616165626002072\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.26287839 11.23186992  6.73080337 11.95537491 18.19854563]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475]reward -9.197430218408051\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.6611,  4.8145,\n",
            "         15.2336, 31.5621, 22.3774]])\n",
            "\n",
            "Task 20.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.6611,  4.8145,\n",
            "         15.2336, 31.5621, 22.3774]])\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -8.116656477314628\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.66110203  4.81445679 15.23363392 31.56209419 22.37739475][ 1.          0.          1.          0.          1.         40.\n",
            "  2.91414046 12.37011579  5.60402756 17.32511297 13.17143703]\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.481145180219736\n",
            "Task 40.0\n",
            "w01 episode  1957 reward -11.5\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.91414046 12.37011579  5.60402756 17.32511297 13.17143703]channels [0.661102030836757, 4.814456789154843, 15.23363392048714, 31.562094191951136, 22.37739475364708]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.91414046  12.37011579   5.60402756  17.32511297\n",
            "  13.17143703]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.9141, 12.3701,\n",
            "          5.6040, 17.3251, 13.1714]])\n",
            "\n",
            "action 4 w00\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.91414046 12.37011579  5.60402756 17.32511297 13.17143703]\n",
            "local_CPU 0\n",
            "reward -11.249659899325236\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.91414046  12.37011579   5.60402756  17.32511297\n",
            "  13.17143703][ 1.          0.          1.          0.          1.         20.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.9141,\n",
            "          12.3701,   5.6040,  17.3251,  13.1714]])\n",
            "\n",
            "Task 20.0\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.91414046  12.37011579   5.60402756  17.32511297\n",
            "  13.17143703]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]local_CPU 0\n",
            "reward -2.6439738415749425\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  7.7204,  9.4033,\n",
            "          6.4841, 30.0740, 46.0479]])\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]\n",
            "Task 80.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]reward -13.437103353857193\n",
            "\n",
            "w00 episode  1958 reward -13.4\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  7.7204,  9.4033,\n",
            "          6.4841, 30.0740, 46.0479]])channels [7.720445414054068, 9.403285620934437, 6.48407811541718, 30.0740212081642, 46.04785523229166]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.26714964   3.70796511   3.231385    10.10590978\n",
            "  56.12303687]\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  7.72044541  9.40328562  6.48407812 30.07402121 46.04785523]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.26714964   3.70796511   3.231385    10.10590978\n",
            "  56.12303687]local_CPU 0\n",
            "\n",
            "reward -4.753066253683393\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.2671,\n",
            "           3.7080,   3.2314,  10.1059,  56.1230]])[ 1.          0.          1.          0.          1.         60.\n",
            "  3.26714964  3.70796511  3.231385   10.10590978 56.12303687]\n",
            "\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.26714964   3.70796511   3.231385    10.10590978\n",
            "  56.12303687]Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.26714964  3.70796511  3.231385   10.10590978 56.12303687]\n",
            "local_CPU 0\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.2671,  3.7080,\n",
            "          3.2314, 10.1059, 56.1230]])reward -2.0942190379160412\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.26714964  3.70796511  3.231385   10.10590978 56.12303687][ 1.          0.          1.          0.          1.         80.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]local_CPU 0\n",
            "reward -6.767093728796748\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.3196,  8.0183,\n",
            "          9.1243, 45.2441, 10.7145]])Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]\n",
            "local_CPU 0\n",
            "reward -5.829919311596894\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]\n",
            "\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.3196,  8.0183,\n",
            "          9.1243, 45.2441, 10.7145]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.31956913  8.01825588  9.12428541 45.24411866 10.7145017 ]\n",
            "\n",
            "local_CPU 0\n",
            "reward -9.813578760978302\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.2644,  6.3910,\n",
            "          7.1542, 34.0482, 30.0328]])\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.2644,  6.3910,\n",
            "          7.1542, 34.0482, 30.0328]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -11.95990881071404\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.26435231  6.39100741  7.15422547 34.04824784 30.03282364]w01 episode  1959 reward -12.0\n",
            "channels [4.264352312611493, 6.391007405118655, 7.154225467606953, 34.04824784495845, 30.03282364266331]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.3750086    5.22711642   5.33510157  16.33412313\n",
            "  32.08123472]\n",
            "Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.3750086    5.22711642   5.33510157  16.33412313\n",
            "  32.08123472]local_CPU 0\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.3750,\n",
            "           5.2271,   5.3351,  16.3341,  32.0812]])\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.3750086    5.22711642   5.33510157  16.33412313\n",
            "  32.08123472]\n",
            "local_CPU 0\n",
            "reward -2.1504878547929267\n",
            "reward -8.13196345550449\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721][ 1.          0.          1.          0.          1.         40.\n",
            "  1.3750086   5.22711642  5.33510157 16.33412313 32.08123472]\n",
            "\n",
            "Task 80.0\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.6743,  9.1183,\n",
            "          6.1327, 40.0921, 30.2364]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.3750086   5.22711642  5.33510157 16.33412313 32.08123472]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.3750,  5.2271,\n",
            "          5.3351, 16.3341, 32.0812]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.3750086   5.22711642  5.33510157 16.33412313 32.08123472]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721]\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.399814752735516\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.88721588  4.42922931  4.77953321 25.9515268  28.80916001]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.88721588  4.42922931  4.77953321 25.9515268  28.80916001]local_CPU 0\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.8872,  4.4292,\n",
            "          4.7795, 25.9515, 28.8092]])\n",
            "reward -10.419459096800171\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.88721588  4.42922931  4.77953321 25.9515268  28.80916001][ 1.          0.          1.          0.          1.         20.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721]\n",
            "\n",
            "local_CPU 0\n",
            "Task 20.0\n",
            "reward -6.584883699427148\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.1021,  2.9309,\n",
            "         12.1336, 40.8130, 63.8428]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.6743,  9.1183,\n",
            "          6.1327, 40.0921, 30.2364]])\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.67430307  9.11827857  6.13271885 40.09213074 30.23642721]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -8.63544934665191\n",
            "reward -12.797365897943136\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ]w00 episode  1960 reward -12.8\n",
            "channels [0.6743030678590944, 9.118278573991471, 6.132718853860362, 40.09213073727281, 30.236427214710794]\n",
            "\n",
            "Task 20.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.88721588   4.42922931   4.77953321  25.9515268\n",
            "  28.80916001]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.88721588   4.42922931   4.77953321  25.9515268\n",
            "  28.80916001]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.8255,  4.5620,\n",
            "          4.8047, 39.8734, 64.9624]])\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.8872,\n",
            "           4.4292,   4.7795,  25.9515,  28.8092]])action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ]action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.88721588   4.42922931   4.77953321  25.9515268\n",
            "  28.80916001]local_CPU 0\n",
            "\n",
            "reward -10.663556706797293\n",
            "local_CPU 0\n",
            "w01 episode  1961 reward -10.7\n",
            "reward -2.34085671773819\n",
            "channels [3.8255430872267104, 4.561953264794086, 4.804658074541346, 39.8734417977183, 64.96244200248907]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.61464391   6.18069905   4.22785321  30.59434089\n",
            "  37.217687  ][ 1.          0.          1.          0.          1.         80.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.61464391   6.18069905   4.22785321  30.59434089\n",
            "  37.217687  ]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.1021,  2.9309,\n",
            "         12.1336, 40.8130, 63.8428]])\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.6146,\n",
            "           6.1807,   4.2279,  30.5943,  37.2177]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.10209974  2.93086838 12.13355394 40.81302986 63.84282906]\n",
            "action 4 w01\n",
            "\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.61464391   6.18069905   4.22785321  30.59434089\n",
            "  37.217687  ]\n",
            "reward -4.4443686377857095\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ]reward -2.210538893390124\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]\n",
            "\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.5660,  7.9338,\n",
            "          7.0954, 44.8479, 16.5065]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ]\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.8255,  4.5620,\n",
            "          4.8047, 39.8734, 64.9624]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -4.7683369476614255\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.82554309  4.56195326  4.80465807 39.8734418  64.962442  ][ 1.          0.          1.          0.          1.         60.\n",
            "  3.61398623  5.65911913  9.04935881 34.85541862 38.00586708]\n",
            "\n",
            "local_CPU 0\n",
            "Task 60.0\n",
            "reward -6.529723978534486\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.61464391  6.18069905  4.22785321 30.59434089 37.217687  ]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.61398623  5.65911913  9.04935881 34.85541862 38.00586708]\n",
            "\n",
            "Task 40.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.6140,  5.6591,\n",
            "          9.0494, 34.8554, 38.0059]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.61464391  6.18069905  4.22785321 30.59434089 37.217687  ]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.6146,  6.1807,\n",
            "          4.2279, 30.5943, 37.2177]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.61398623  5.65911913  9.04935881 34.85541862 38.00586708]\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.61464391  6.18069905  4.22785321 30.59434089 37.217687  ]reward -6.8824727797261875\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]local_CPU 0\n",
            "reward -8.832389061627818\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.5660,  7.9338,\n",
            "          7.0954, 44.8479, 16.5065]])Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.1248,  4.6413,\n",
            "          8.9036, 25.8360, 36.3748]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.56599102  7.93381131  7.09535506 44.84786777 16.50645689]\n",
            "local_CPU 0\n",
            "\n",
            "reward -11.715064235688088\n",
            "action 4 w01\n",
            "w00 episode  1962 reward -11.7\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]channels [2.5659910155826995, 7.933811306193596, 7.095355062623315, 44.84786777222351, 16.506456887405474]\n",
            "\n",
            "local_CPU 0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.61398623   5.65911913   9.04935881  34.85541862\n",
            "  38.00586708]reward -9.05703054598252\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]\n",
            "\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.61398623   5.65911913   9.04935881  34.85541862\n",
            "  38.00586708]Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.6140,\n",
            "           5.6591,   9.0494,  34.8554,  38.0059]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.6011, 20.2280,\n",
            "          3.8724, 34.1572, 15.8230]])action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.61398623   5.65911913   9.04935881  34.85541862\n",
            "  38.00586708]\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]reward -2.2241675025094083\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]local_CPU 0\n",
            "\n",
            "reward -11.853547047763653\n",
            "Task 80.0\n",
            "w01 episode  1963 reward -11.9\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]channels [3.6011242144023523, 20.228036112778877, 3.872353546231376, 34.15724089173013, 15.822962041469221]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.55221474  13.42085753  12.06261779  30.62256259\n",
            "  41.54424523]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.1248,  4.6413,\n",
            "          8.9036, 25.8360, 36.3748]])\n",
            "action 4 w00\n",
            "\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.12477951  4.64126247  8.90361242 25.83600124 36.37480556]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.55221474  13.42085753  12.06261779  30.62256259\n",
            "  41.54424523]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.5522,\n",
            "          13.4209,  12.0626,  30.6226,  41.5442]])\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.55221474  13.42085753  12.06261779  30.62256259\n",
            "  41.54424523]reward -4.50190870576343\n",
            "\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]reward -2.093809116889537\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]Task 60.0\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.6011, 20.2280,\n",
            "          3.8724, 34.1572, 15.8230]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.3332,  6.5493,\n",
            "          4.9640, 55.0640, 63.4476]])action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.60112421 20.22803611  3.87235355 34.15724089 15.82296204]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]local_CPU 0\n",
            "\n",
            "reward -7.629861730183771\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.55221474 13.42085753 12.06261779 30.62256259 41.54424523]reward -4.089095713702123\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.55221474 13.42085753 12.06261779 30.62256259 41.54424523][ 1.          0.          1.          0.          1.         60.\n",
            "  2.60467428  8.62222788  7.57031166 23.00419902 53.55069651]\n",
            "\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.5522, 13.4209,\n",
            "         12.0626, 30.6226, 41.5442]])\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.60467428  8.62222788  7.57031166 23.00419902 53.55069651]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.6047,  8.6222,\n",
            "          7.5703, 23.0042, 53.5507]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.55221474 13.42085753 12.06261779 30.62256259 41.54424523]\n",
            "action 4 w01\n",
            "\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.60467428  8.62222788  7.57031166 23.00419902 53.55069651]\n",
            "reward -9.823041361852134\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]reward -6.147274254861875\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]\n",
            "\n",
            "Task 40.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.3332,  6.5493,\n",
            "          4.9640, 55.0640, 63.4476]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.9946,  7.2161,\n",
            "          3.8524, 23.7865, 11.4898]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.33323059  6.54926391  4.96400015 55.06401155 63.44756061]\n",
            "local_CPU 0\n",
            "reward -11.890052396444695\n",
            "w00 episode  1964 reward -11.9\n",
            "channels [1.333230594990351, 6.549263908475068, 4.964000147551511, 55.06401155212067, 63.447560611343114]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.60467428   8.62222788   7.57031166  23.00419902\n",
            "  53.55069651]\n",
            "action 4 w01\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]\n",
            "local_CPU 0\n",
            "reward -9.520936316840432\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.60467428   8.62222788   7.57031166  23.00419902\n",
            "  53.55069651]Task 20.0\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.6047,\n",
            "           8.6222,   7.5703,  23.0042,  53.5507]])\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.60467428   8.62222788   7.57031166  23.00419902\n",
            "  53.55069651]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.8327,  3.6277,\n",
            "          7.5341, 11.0160, 16.5146]])\n",
            "local_CPU 0\n",
            "reward -2.1292692939112143\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.9946,  7.2161,\n",
            "          3.8524, 23.7865, 11.4898]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]\n",
            "local_CPU 0\n",
            "reward -11.947893746964166\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.99463077  7.21605199  3.85243544 23.78652322 11.4898392 ]w01 episode  1965 reward -11.9\n",
            "\n",
            "local_CPU 0\n",
            "channels [0.8326832516084454, 3.6276894225704384, 7.5341122914305885, 11.01598285321587, 16.514560861793687]\n",
            "reward -4.341922898897753\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.54265578   2.39236276  10.31932175   4.25875146\n",
            "  35.96690898]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.8327,  3.6277,\n",
            "          7.5341, 11.0160, 16.5146]])\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.54265578   2.39236276  10.31932175   4.25875146\n",
            "  35.96690898]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.5427,\n",
            "           2.3924,  10.3193,   4.2588,  35.9669]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.54265578   2.39236276  10.31932175   4.25875146\n",
            "  35.96690898]\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.83268325  3.62768942  7.53411229 11.01598285 16.51456086]reward -2.2187416495090533\n",
            "\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067]reward -7.112511139531755\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.54265578  2.39236276 10.31932175  4.25875146 35.96690898]\n",
            "Task 80.0\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.54265578  2.39236276 10.31932175  4.25875146 35.96690898]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.5735,  7.6947,\n",
            "         12.9278, 40.9048, 66.8150]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.5427,  2.3924,\n",
            "         10.3193,  4.2588, 35.9669]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -4.2475690830832775\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.54265578  2.39236276 10.31932175  4.25875146 35.96690898]\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.51831203 12.69536818  5.27502342 11.59884713 20.4736219 ]\n",
            "reward -9.42819011917554\n",
            "Task 60.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.51831203 12.69536818  5.27502342 11.59884713 20.4736219 ]\n",
            "\n",
            "Task 20.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.5183, 12.6954,\n",
            "          5.2750, 11.5988, 20.4736]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.5735,  7.6947,\n",
            "         12.9278, 40.9048, 66.8150]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.51831203 12.69536818  5.27502342 11.59884713 20.4736219 ]\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -6.6169361280286125\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.57350041  7.69467517 12.92778149 40.90484649 66.81500067][ 1.          0.          1.          0.          1.         40.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.511144096298633\n",
            "Task 40.0\n",
            "w00 episode  1966 reward -11.5\n",
            "channels [3.5735004101834575, 7.694675173965402, 12.927781489111851, 40.90484649317182, 66.81500067233439]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.51831203  12.69536818   5.27502342  11.59884713\n",
            "  20.4736219 ]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.51831203  12.69536818   5.27502342  11.59884713\n",
            "  20.4736219 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.5636, 11.9844,\n",
            "         12.5968, 36.8291, 13.3958]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.5183,\n",
            "          12.6954,   5.2750,  11.5988,  20.4736]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.51831203  12.69536818   5.27502342  11.59884713\n",
            "  20.4736219 ]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.606847653659555\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]reward -9.379591208808876\n",
            "\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.5636, 11.9844,\n",
            "         12.5968, 36.8291, 13.3958]])Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.2806,  1.7890,\n",
            "          9.1409, 32.3059, 69.6222]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.56357346 11.98444854 12.59681814 36.82911014 13.3957792 ]reward -11.392274930323607\n",
            "\n",
            "w01 episode  1967 reward -11.4\n",
            "local_CPU 0\n",
            "channels [3.280629631734257, 1.789004166952476, 9.140944552726342, 32.30585560767734, 69.6222265901748]\n",
            "reward -5.831146219361668\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           0.38681172  10.56082334  11.89820046  39.53413416\n",
            "  17.23922827][ 1.          0.          1.          0.          1.         60.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]\n",
            "\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.38681172  10.56082334  11.89820046  39.53413416\n",
            "  17.23922827]Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.3868,\n",
            "          10.5608,  11.8982,  39.5341,  17.2392]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.2806,  1.7890,\n",
            "          9.1409, 32.3059, 69.6222]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.28062963  1.78900417  9.14094455 32.30585561 69.62222659]\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.38681172  10.56082334  11.89820046  39.53413416\n",
            "  17.23922827]\n",
            "local_CPU 0\n",
            "reward -7.898645057117438\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.38681172 10.56082334 11.89820046 39.53413416 17.23922827]reward -2.433152291354597\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "Task 40.0\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.38681172 10.56082334 11.89820046 39.53413416 17.23922827]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.3868, 10.5608,\n",
            "         11.8982, 39.5341, 17.2392]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "action 4 w00\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.0481, 11.8599,\n",
            "         16.5703, 23.7352, 19.7779]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.38681172 10.56082334 11.89820046 39.53413416 17.23922827]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "local_CPU 0\n",
            "\n",
            "reward -10.647611323805249\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "local_CPU 0\n",
            "reward -4.726521149598562\n",
            "Task 20.0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.16964243  2.12150366  3.81834589 11.27960706 12.055108  ]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.16964243  2.12150366  3.81834589 11.27960706 12.055108  ]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.0481, 11.8599,\n",
            "         16.5703, 23.7352, 19.7779]])\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.1696,  2.1215,\n",
            "          3.8183, 11.2796, 12.0551]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.04810178 11.85993623 16.57025725 23.73524134 19.77785818]\n",
            "local_CPU 0\n",
            "reward -13.210764776862094\n",
            "\n",
            "w00 episode  1968 reward -13.2\n",
            "action 4 w01\n",
            "channels [3.0481017821099283, 11.859936230826708, 16.570257251541612, 23.73524134479962, 19.777858181180473]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           0.16964243   2.12150366   3.81834589  11.27960706\n",
            "  12.055108  ]\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.16964243  2.12150366  3.81834589 11.27960706 12.055108  ]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.16964243   2.12150366   3.81834589  11.27960706\n",
            "  12.055108  ]local_CPU 0\n",
            "\n",
            "reward -7.363717557580683\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.1696,\n",
            "           2.1215,   3.8183,  11.2796,  12.0551]])[ 1.          0.          1.          0.          1.         40.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]\n",
            "\n",
            "action 4 w00\n",
            "Task 40.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.16964243   2.12150366   3.81834589  11.27960706\n",
            "  12.055108  ]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.6259,  3.6211,\n",
            "          4.8518, 24.2139, 33.1841]])\n",
            "reward -3.2178787180100503\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]\n",
            "Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]local_CPU 0\n",
            "reward -9.518606370103694\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.6259,  3.6211,\n",
            "          4.8518, 24.2139, 33.1841]])Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.62592796  3.62112314  4.85177298 24.21387527 33.18408111]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.8553,  4.2690,\n",
            "          3.6506, 61.0161, 44.4415]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -5.500358094253446\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117][ 1.          0.          1.          0.          1.         60.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117]\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117]local_CPU 0\n",
            "reward -11.648397714695177\n",
            "\n",
            "w01 episode  1969 reward -11.6\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.8553,  4.2690,\n",
            "          3.6506, 61.0161, 44.4415]])channels [1.8552570296539652, 4.269010390280648, 3.650552000737847, 61.01614044891152, 44.441481165271036]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.13181871   4.29552341   4.81279307  55.80576208\n",
            "  15.52899018]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.85525703  4.26901039  3.650552   61.01614045 44.44148117]Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.13181871   4.29552341   4.81279307  55.80576208\n",
            "  15.52899018]local_CPU 0\n",
            "reward -7.708982775539457\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.13181871  4.29552341  4.81279307 55.80576208 15.52899018]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.1318,\n",
            "           4.2955,   4.8128,  55.8058,  15.5290]])\n",
            "Task 40.0\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.13181871  4.29552341  4.81279307 55.80576208 15.52899018]observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.13181871   4.29552341   4.81279307  55.80576208\n",
            "  15.52899018]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.1318,  4.2955,\n",
            "          4.8128, 55.8058, 15.5290]])\n",
            "reward -2.6523391760219166\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.13181871  4.29552341  4.81279307 55.80576208 15.52899018]\n",
            "Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]local_CPU 0\n",
            "reward -10.716351961799758\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.8955,  2.4334,\n",
            "          2.9779, 26.3726, 25.0267]])\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]\n",
            "Task 20.0\n",
            "local_CPU 0\n",
            "reward -4.854654958094536\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.8955,  2.4334,\n",
            "          2.9779, 26.3726, 25.0267]])[ 1.          0.          1.          0.          1.         60.\n",
            "  4.01169899  7.93778993  9.98910287 16.23860705 28.42311211]\n",
            "\n",
            "Task 60.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.89547904  2.43343323  2.97794469 26.37258341 25.02670717]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.01169899  7.93778993  9.98910287 16.23860705 28.42311211]\n",
            "\n",
            "local_CPU 0\n",
            "reward -13.114330873624759\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.0117,  7.9378,\n",
            "          9.9891, 16.2386, 28.4231]])w00 episode  1970 reward -13.1\n",
            "channels [2.8954790412594744, 2.433433230436132, 2.9779446889057324, 26.372583406857526, 25.02670716686859]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           4.01169899   7.93778993   9.98910287  16.23860705\n",
            "  28.42311211]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.01169899  7.93778993  9.98910287 16.23860705 28.42311211]Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.01169899   7.93778993   9.98910287  16.23860705\n",
            "  28.42311211]\n",
            "local_CPU 0\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.0117,\n",
            "           7.9378,   9.9891,  16.2386,  28.4231]])reward -7.0781469173555704\n",
            "\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.01169899   7.93778993   9.98910287  16.23860705\n",
            "  28.42311211][ 1.          0.          1.          0.          1.         40.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508]\n",
            "local_CPU 0\n",
            "\n",
            "Task 40.0\n",
            "reward -2.3734670250584182\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508][ 1.          0.          1.          0.          1.         80.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.2343,  4.0924,\n",
            "          7.3221, 20.8335, 62.3613]])\n",
            "Task 80.0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508]\n",
            "local_CPU 0\n",
            "reward -9.09531303072098\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508]\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.2343,  4.0924,\n",
            "          7.3221, 20.8335, 62.3613]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.23427855  4.09242999  7.32214547 20.83347023 62.36132508]\n",
            "local_CPU 0\n",
            "reward -4.456011881107287\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.6366,  5.3813,\n",
            "          8.6302, 39.6810, 40.2321]])\n",
            "action 4 w00\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.6366,  5.3813,\n",
            "          8.6302, 39.6810, 40.2321]])\n",
            "local_CPU 0\n",
            "reward -6.650879977189264\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.63656088  5.38126148  8.63020713 39.6810332  40.23211907][ 1.          0.          1.          0.          1.         40.\n",
            "  3.46896599 14.37320228  5.65440811 20.32456087 56.36730756]\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.46896599 14.37320228  5.65440811 20.32456087 56.36730756]local_CPU 0\n",
            "\n",
            "reward -11.182741042574696\n",
            "w01 episode  1971 reward -11.2\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.4690, 14.3732,\n",
            "          5.6544, 20.3246, 56.3673]])channels [1.6365608807653178, 5.3812614767510105, 8.630207125842071, 39.68103320127488, 40.23211907382778]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.46896599  14.37320228   5.65440811  20.32456087\n",
            "  56.36730756]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.46896599  14.37320228   5.65440811  20.32456087\n",
            "  56.36730756]\n",
            "action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.46896599 14.37320228  5.65440811 20.32456087 56.36730756]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.4690,\n",
            "          14.3732,   5.6544,  20.3246,  56.3673]])\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.46896599  14.37320228   5.65440811  20.32456087\n",
            "  56.36730756]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.0133332696825494\n",
            "reward -8.744081241070175\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.5853,  9.1391,\n",
            "          4.8004, 36.2756, 33.3463]])[ 1.          0.          1.          0.          1.         80.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]\n",
            "\n",
            "Task 80.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.5853,  9.1391,\n",
            "          4.8004, 36.2756, 33.3463]])local_CPU 0\n",
            "\n",
            "reward -11.027060745450354\n",
            "action 4 w01\n",
            "w00 episode  1972 reward -11.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.58528865  9.13914876  4.80039776 36.27556184 33.34629305]channels [3.5852886523814442, 9.13914875651117, 4.800397760643242, 36.275561837662295, 33.34629304812852]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.47989424   6.53685533   5.42860237  60.7755865\n",
            "  28.80533014]\n",
            "local_CPU 0\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.47989424   6.53685533   5.42860237  60.7755865\n",
            "  28.80533014]\n",
            "reward -4.170470845027013\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.4799,\n",
            "           6.5369,   5.4286,  60.7756,  28.8053]])[ 1.          0.          1.          0.          1.         60.\n",
            "  3.47989424  6.53685533  5.42860237 60.7755865  28.80533014]\n",
            "Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.47989424  6.53685533  5.42860237 60.7755865  28.80533014]action 4 w00\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.4799,  6.5369,\n",
            "          5.4286, 60.7756, 28.8053]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.47989424   6.53685533   5.42860237  60.7755865\n",
            "  28.80533014]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.47989424  6.53685533  5.42860237 60.7755865  28.80533014]\n",
            "local_CPU 0\n",
            "\n",
            "reward -2.421332626605919\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637]\n",
            "reward -6.457527439005908\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637][ 1.          0.          1.          0.          1.         40.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637]\n",
            "Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.0417,  8.7891,\n",
            "          7.1449, 12.9781, 37.2355]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.0417,  8.7891,\n",
            "          7.1449, 12.9781, 37.2355]])\n",
            "local_CPU 0\n",
            "reward -4.6351148908449655\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.0416798   8.7890726   7.14489234 12.97805326 37.23553637]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.7395,  6.4242,\n",
            "         11.6897, 26.9440, 51.5754]])\n",
            "\n",
            "local_CPU 0\n",
            "reward -8.54976961063852\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]\n",
            "\n",
            "Task 20.0\n",
            "local_CPU 0\n",
            "reward -6.7680293031313425\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.30693773  8.67980948  3.63601574 21.51602655 30.58705479]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.30693773  8.67980948  3.63601574 21.51602655 30.58705479]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.7395,  6.4242,\n",
            "         11.6897, 26.9440, 51.5754]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.3069,  8.6798,\n",
            "          3.6360, 21.5160, 30.5871]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.73945704  6.42420263 11.68972865 26.94403198 51.57544787]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.30693773  8.67980948  3.63601574 21.51602655 30.58705479]\n",
            "local_CPU 0\n",
            "\n",
            "reward -10.60585906408334\n",
            "local_CPU 0\n",
            "w01 episode  1973 reward -10.6\n",
            "reward -9.081368203802537\n",
            "channels [0.7394570446602047, 6.424202634703324, 11.689728651665517, 26.944031975286453, 51.57544787210825]\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]\n",
            "Task 20.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.30693773   8.67980948   3.63601574  21.51602655\n",
            "  30.58705479]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.0392,  8.3203,\n",
            "          3.6641,  6.2237, 41.9534]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.30693773   8.67980948   3.63601574  21.51602655\n",
            "  30.58705479]\n",
            "action 4 w00\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.3069,\n",
            "           8.6798,   3.6360,  21.5160,  30.5871]])\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]\n",
            "local_CPU 0\n",
            "reward -11.276212691277673\n",
            "w00 episode  1974 reward -11.3\n",
            "channels [2.039151557509146, 8.320251081483228, 3.664052812638108, 6.223726225223777, 41.953422762742214]\n",
            "action 4 w01\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           4.05394534   7.03170238   0.90770424  14.92366771\n",
            "  30.58729717]\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.30693773   8.67980948   3.63601574  21.51602655\n",
            "  30.58705479]Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.05394534   7.03170238   0.90770424  14.92366771\n",
            "  30.58729717]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.0539,\n",
            "           7.0317,   0.9077,  14.9237,  30.5873]])\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.05394534   7.03170238   0.90770424  14.92366771\n",
            "  30.58729717]\n",
            "local_CPU 0\n",
            "reward -2.349798436327466\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]\n",
            "reward -2.16964963793988\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]\n",
            "Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  5.1416, 10.4457,\n",
            "          2.4769,  1.8241,  8.8451]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.0392,  8.3203,\n",
            "          3.6641,  6.2237, 41.9534]])action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.03915156  8.32025108  3.66405281  6.22372623 41.95342276]reward -4.789655124916491\n",
            "\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ]reward -4.268366364197449\n",
            "\n",
            "Task 60.0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.05394534  7.03170238  0.90770424 14.92366771 30.58729717]\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ]Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.05394534  7.03170238  0.90770424 14.92366771 30.58729717]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.2018,  2.7361,\n",
            "          4.5412, 29.2153, 13.0341]])\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.0539,  7.0317,\n",
            "          0.9077, 14.9237, 30.5873]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.05394534  7.03170238  0.90770424 14.92366771 30.58729717]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -7.841020975914004\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.51165159  2.33715894  1.94277217 25.53335021 57.35511038]reward -6.48625307937092\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.51165159  2.33715894  1.94277217 25.53335021 57.35511038]\n",
            "Task 40.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.5117,  2.3372,\n",
            "          1.9428, 25.5333, 57.3551]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  5.1416, 10.4457,\n",
            "          2.4769,  1.8241,  8.8451]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.14157298 10.4456905   2.47690012  1.82408396  8.84505731]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.51165159  2.33715894  1.94277217 25.53335021 57.35511038]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -10.240297556349567\n",
            "reward -9.936176717463798\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ][ 1.          0.          1.          0.          1.         20.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.2018,  2.7361,\n",
            "          4.5412, 29.2153, 13.0341]])\n",
            "\n",
            "action 4 w01\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.20181722  2.73609603  4.54123733 29.21528334 13.0341169 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.0287, 10.9934,\n",
            "          6.0472, 83.3765, 11.5564]])local_CPU 0\n",
            "reward -12.777819777921776\n",
            "\n",
            "w01 episode  1975 reward -12.8\n",
            "action 4 w00\n",
            "channels [1.2018172230519684, 2.736096033883282, 4.541237330041261, 29.215283338685392, 13.034116900360544]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691][  1.           0.           1.           0.           1.\n",
            " 100.           4.51165159   2.33715894   1.94277217  25.53335021\n",
            "  57.35511038]\n",
            "local_CPU 0\n",
            "\n",
            "reward -13.680639765612906\n",
            "Task 100.0\n",
            "w00 episode  1976 reward -13.7\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.51165159   2.33715894   1.94277217  25.53335021\n",
            "  57.35511038]channels [2.0286747010186237, 10.99337107673764, 6.0471776898445935, 83.37653940034592, 11.556366911028512]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.55048352   5.03528986   8.99007825  52.59041709\n",
            "  31.09734653]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.5117,\n",
            "           2.3372,   1.9428,  25.5333,  57.3551]])Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.55048352   5.03528986   8.99007825  52.59041709\n",
            "  31.09734653]action 4 w01\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.51165159   2.33715894   1.94277217  25.53335021\n",
            "  57.35511038]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.5505,\n",
            "           5.0353,   8.9901,  52.5904,  31.0973]])\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.0204810234647166\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.55048352   5.03528986   8.99007825  52.59041709\n",
            "  31.09734653][ 1.          0.          1.          0.          1.         80.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691]\n",
            "\n",
            "local_CPU 0\n",
            "Task 80.0\n",
            "reward -2.2976394499985737\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691]\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.0287, 10.9934,\n",
            "          6.0472, 83.3765, 11.5564]])\n",
            "\n",
            "action 4 w01\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.0286747  10.99337108  6.04717769 83.3765394  11.55636691]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.8288,  0.9872,\n",
            "          7.0663, 40.5615, 38.0693]])\n",
            "reward -5.176069777215462\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.55048352  5.03528986  8.99007825 52.59041709 31.09734653]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.55048352  5.03528986  8.99007825 52.59041709 31.09734653]reward -4.538524530074852\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.5505,  5.0353,\n",
            "          8.9901, 52.5904, 31.0973]])\n",
            "\n",
            "Task 60.0\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.55048352  5.03528986  8.99007825 52.59041709 31.09734653]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.3391,  4.6712,\n",
            "          2.3831, 53.8467, 24.2852]])local_CPU 0\n",
            "reward -7.329696599665571\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.8288,  0.9872,\n",
            "          7.0663, 40.5615, 38.0693]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.82876613  0.98720132  7.06627453 40.56147667 38.06933899]\n",
            "\n",
            "local_CPU 0\n",
            "reward -9.46749251618677\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]reward -6.954684459478422\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.88292801  4.25860357  9.39658653 19.93017208 26.08708207]\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.88292801  4.25860357  9.39658653 19.93017208 26.08708207]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.8829,  4.2586,\n",
            "          9.3966, 19.9302, 26.0871]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.88292801  4.25860357  9.39658653 19.93017208 26.08708207]\n",
            "local_CPU 0\n",
            "reward -9.333389614723684\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.3391,  4.6712,\n",
            "          2.3831, 53.8467, 24.2852]])Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.7648,  6.4709,\n",
            "          9.1037, 20.8785, 37.2994]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]\n",
            "local_CPU 0\n",
            "reward -11.617189522543187\n",
            "\n",
            "action 4 w01\n",
            "w00 episode  1977 reward -11.6\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.33905791  4.67116541  2.38305922 53.84669288 24.28517803]channels [4.764755931191856, 6.470863871896015, 9.103714758642319, 20.87849429387546, 37.29940861904961]\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.680134950749384\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.44435664  10.2336088    9.94220003  29.30254157\n",
            "  21.40386816]w01 episode  1978 reward -11.7\n",
            "\n",
            "channels [4.339057910611323, 4.671165412467466, 2.3830592166364855, 53.846692884599264, 24.285178025295256]\n",
            "Task 100.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.88292801   4.25860357   9.39658653  19.93017208\n",
            "  26.08708207]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.44435664  10.2336088    9.94220003  29.30254157\n",
            "  21.40386816]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.4444,\n",
            "          10.2336,   9.9422,  29.3025,  21.4039]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.88292801   4.25860357   9.39658653  19.93017208\n",
            "  26.08708207]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.8829,\n",
            "           4.2586,   9.3966,  19.9302,  26.0871]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.44435664  10.2336088    9.94220003  29.30254157\n",
            "  21.40386816]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.88292801   4.25860357   9.39658653  19.93017208\n",
            "  26.08708207]reward -2.5496555538415904\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]local_CPU 0\n",
            "\n",
            "reward -2.1952034706132424\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]\n",
            "\n",
            "Task 80.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.1849,  3.6979,\n",
            "         14.8347, 40.1953, 81.3828]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.7648,  6.4709,\n",
            "          9.1037, 20.8785, 37.2994]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.76475593  6.47086387  9.10371476 20.87849429 37.29940862]reward -4.570376272404859\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]local_CPU 0\n",
            "\n",
            "reward -4.383269589444387\n",
            "Task 60.0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.44435664 10.2336088   9.94220003 29.30254157 21.40386816]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]\n",
            "\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.7825,  4.6838,\n",
            "         10.0010, 25.5313, 58.4665]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.44435664 10.2336088   9.94220003 29.30254157 21.40386816]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.4444, 10.2336,\n",
            "          9.9422, 29.3025, 21.4039]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.44435664 10.2336088   9.94220003 29.30254157 21.40386816]reward -6.667770701788349\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  7.35453904  8.74796957  0.8191406  18.20039537 50.93194973]local_CPU 0\n",
            "\n",
            "reward -6.7061233910560185\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  7.35453904  8.74796957  0.8191406  18.20039537 50.93194973]\n",
            "\n",
            "Task 40.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  7.3545,  8.7480,\n",
            "          0.8191, 18.2004, 50.9319]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.1849,  3.6979,\n",
            "         14.8347, 40.1953, 81.3828]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  7.35453904  8.74796957  0.8191406  18.20039537 50.93194973]\n",
            "\n",
            "action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.18489782  3.69786354 14.83465292 40.19530243 81.38277564]reward -8.812659475419975\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]local_CPU 0\n",
            "\n",
            "reward -8.669196725291478\n",
            "Task 20.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.5222,  7.9532,\n",
            "          5.8974, 22.3986, 16.5782]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.7825,  4.6838,\n",
            "         10.0010, 25.5313, 58.4665]])local_CPU 0\n",
            "\n",
            "reward -11.716267011187112\n",
            "action 4 w01\n",
            "w00 episode  1979 reward -11.7\n",
            "channels [2.522214944515457, 7.953201311916816, 5.897377379440524, 22.398614678458884, 16.578214685892682]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.04935886   4.43067926   5.94029263  30.7149607\n",
            "  24.09912127]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.78246845  4.68375365 10.0010122  25.53134519 58.46648877]\n",
            "\n",
            "local_CPU 0\n",
            "reward -10.697074964297753\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.04935886   4.43067926   5.94029263  30.7149607\n",
            "  24.09912127]\n",
            "w01 episode  1980 reward -10.7\n",
            "channels [4.782468453484025, 4.683753654855144, 10.001012197185323, 25.531345189029288, 58.466488773018625]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           7.35453904   8.74796957   0.8191406   18.20039537\n",
            "  50.93194973]\n",
            "Task 100.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.0494,\n",
            "           4.4307,   5.9403,  30.7150,  24.0991]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           7.35453904   8.74796957   0.8191406   18.20039537\n",
            "  50.93194973]action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.04935886   4.43067926   5.94029263  30.7149607\n",
            "  24.09912127]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   7.3545,\n",
            "           8.7480,   0.8191,  18.2004,  50.9319]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           7.35453904   8.74796957   0.8191406   18.20039537\n",
            "  50.93194973]reward -2.4964739946593277\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]local_CPU 0\n",
            "reward -2.0707328821330786\n",
            "\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]\n",
            "Task 80.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.7054,  6.6516,\n",
            "          5.1037, 43.1337, 39.1840]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]\n",
            "action 4 w00\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.5222,  7.9532,\n",
            "          5.8974, 22.3986, 16.5782]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.743616883742799\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822]action 4 w01\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.3567,  4.7173,\n",
            "          6.8435, 22.5722, 39.9661]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.52221494  7.95320131  5.89737738 22.39861468 16.57821469]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822]\n",
            "local_CPU 0\n",
            "reward -6.932868250948804\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  5.12685416  5.0502058   6.22229493 10.7380248  14.22212397]local_CPU 0\n",
            "\n",
            "reward -4.655111842356353\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.04935886  4.43067926  5.94029263 30.7149607  24.09912127]Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.12685416  5.0502058   6.22229493 10.7380248  14.22212397]Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.04935886  4.43067926  5.94029263 30.7149607  24.09912127]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  5.1269,  5.0502,\n",
            "          6.2223, 10.7380, 14.2221]])\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.0494,  4.4307,\n",
            "          5.9403, 30.7150, 24.0991]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.12685416  5.0502058   6.22229493 10.7380248  14.22212397]\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.04935886  4.43067926  5.94029263 30.7149607  24.09912127]local_CPU 0\n",
            "\n",
            "reward -9.943238605459435\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]reward -6.968520640354731\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.4692,  7.7524,\n",
            "          4.5473, 13.9435, 34.2189]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.7054,  6.6516,\n",
            "          5.1037, 43.1337, 39.1840]])action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.70538758  6.65155078  5.1037037  43.13366528 39.18402962]local_CPU 0\n",
            "\n",
            "reward -12.210809008849402\n",
            "local_CPU 0\n",
            "w00 episode  1981 reward -12.2\n",
            "reward -9.12152586956309\n",
            "channels [2.469196060996474, 7.752386766394145, 4.547269881784567, 13.943539104530016, 34.21891641776103]\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822][  1.           0.           1.           0.           1.\n",
            " 100.           2.72424045   5.40955376   5.00754528  35.04928268\n",
            "  10.50371293]\n",
            "\n",
            "Task 20.0\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.72424045   5.40955376   5.00754528  35.04928268\n",
            "  10.50371293]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.3567,  4.7173,\n",
            "          6.8435, 22.5722, 39.9661]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.7242,\n",
            "           5.4096,   5.0075,  35.0493,  10.5037]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.72424045   5.40955376   5.00754528  35.04928268\n",
            "  10.50371293]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.35665648  4.7173366   6.84351146 22.57224155 39.96612822]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.3435620800862824\n",
            "reward -11.19822366785928\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]w01 episode  1982 reward -11.2\n",
            "\n",
            "channels [3.356656482887001, 4.717336600979264, 6.843511458445798, 22.57224154564822, 39.96612822338302]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           5.12685416   5.0502058    6.22229493  10.7380248\n",
            "  14.22212397]Task 80.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.12685416   5.0502058    6.22229493  10.7380248\n",
            "  14.22212397]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.9619, 12.6123,\n",
            "          7.2013, 68.4511, 51.2046]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   5.1269,\n",
            "           5.0502,   6.2223,  10.7380,  14.2221]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.12685416   5.0502058    6.22229493  10.7380248\n",
            "  14.22212397]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.5811717977469657\n",
            "local_CPU 0\n",
            "reward -4.501407379734832\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]\n",
            "Task 60.0\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.7920,  6.0716,\n",
            "          6.2378, 17.8414, 42.7314]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.4692,  7.7524,\n",
            "          4.5473, 13.9435, 34.2189]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.46919606  7.75238677  4.54726988 13.9435391  34.21891642]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]\n",
            "local_CPU 0\n",
            "\n",
            "reward -4.72515365749063\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.72424045  5.40955376  5.00754528 35.04928268 10.50371293]reward -6.755909515472711\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.86469234  4.98613307 12.13369216 22.71493204 97.22031683]Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.72424045  5.40955376  5.00754528 35.04928268 10.50371293]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.86469234  4.98613307 12.13369216 22.71493204 97.22031683]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.7242,  5.4096,\n",
            "          5.0075, 35.0493, 10.5037]])\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.8647,  4.9861,\n",
            "         12.1337, 22.7149, 97.2203]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.72424045  5.40955376  5.00754528 35.04928268 10.50371293]\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.86469234  4.98613307 12.13369216 22.71493204 97.22031683]local_CPU 0\n",
            "\n",
            "reward -7.862098955825385\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]\n",
            "reward -8.748086845228405\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]\n",
            "Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.9619, 12.6123,\n",
            "          7.2013, 68.4511, 51.2046]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.4443,  7.7958,\n",
            "          6.5600,  9.1788, 23.9482]])\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.96194093 12.61234723  7.20134252 68.45110738 51.20457119]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -9.951386694623682\n",
            "reward -11.28244134067084\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]w00 episode  1983 reward -11.3\n",
            "\n",
            "channels [1.444275446980896, 7.795827482694795, 6.559951269596337, 9.178758896708851, 23.948223970476434]\n",
            "Task 20.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.19844636   3.30809913   9.06308221  17.85702307\n",
            "  21.85336596]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.7920,  6.0716,\n",
            "          6.2378, 17.8414, 42.7314]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.19844636   3.30809913   9.06308221  17.85702307\n",
            "  21.85336596]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.1984,\n",
            "           3.3081,   9.0631,  17.8570,  21.8534]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.79203974  6.07157388  6.23779673 17.84137211 42.73141286]\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.19844636   3.30809913   9.06308221  17.85702307\n",
            "  21.85336596]reward -12.130676015524067\n",
            "\n",
            "w01 episode  1984 reward -12.1\n",
            "local_CPU 0\n",
            "channels [1.7920397432514281, 6.071573883428828, 6.2377967349804, 17.841372111452475, 42.73141286385985]\n",
            "reward -2.558069340527867\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.86469234   4.98613307  12.13369216  22.71493204\n",
            "  97.22031683][ 1.          0.          1.          0.          1.         80.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ]\n",
            "\n",
            "Task 100.0\n",
            "Task 80.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.86469234   4.98613307  12.13369216  22.71493204\n",
            "  97.22031683]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.8647,\n",
            "           4.9861,  12.1337,  22.7149,  97.2203]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.2662,  7.1733,\n",
            "          7.3017, 20.6101, 26.9551]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.86469234   4.98613307  12.13369216  22.71493204\n",
            "  97.22031683]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -1.9416453982077526\n",
            "reward -4.954510323415887\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397][ 1.          0.          1.          0.          1.         60.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]\n",
            "\n",
            "Task 60.0\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.6151, 10.9062,\n",
            "         11.4848, 66.4186, 22.9983]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.4443,  7.7958,\n",
            "          6.5600,  9.1788, 23.9482]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.44427545  7.79582748  6.55995127  9.1787589  23.94822397]\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.297810423624101\n",
            "local_CPU 0\n",
            "reward -7.551644663547652\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  5.63009481  5.01012645  7.7552564  17.33250743 16.22688787][ 1.          0.          1.          0.          1.         60.\n",
            "  3.19844636  3.30809913  9.06308221 17.85702307 21.85336596]\n",
            "\n",
            "Task 40.0\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.63009481  5.01012645  7.7552564  17.33250743 16.22688787]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.19844636  3.30809913  9.06308221 17.85702307 21.85336596]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  5.6301,  5.0101,\n",
            "          7.7553, 17.3325, 16.2269]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.1984,  3.3081,\n",
            "          9.0631, 17.8570, 21.8534]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.19844636  3.30809913  9.06308221 17.85702307 21.85336596]\n",
            "local_CPU 0\n",
            "reward -6.641682145279299\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  5.63009481  5.01012645  7.7552564  17.33250743 16.22688787][ 1.          0.          1.          0.          1.         40.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ]\n",
            "\n",
            "local_CPU 0\n",
            "reward -10.340691994281986\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.8288,  3.7668,\n",
            "          2.0465, 28.9770, 27.3834]])\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ]local_CPU 0\n",
            "reward -12.770918237182057\n",
            "w00 episode  1985 reward -12.8\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.2662,  7.1733,\n",
            "          7.3017, 20.6101, 26.9551]])\n",
            "channels [2.8288003926680227, 3.7667860019203405, 2.0465188558904477, 28.976979567916164, 27.383354396173655]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.26624373  7.17327529  7.30166481 20.61008079 26.9550659 ][  1.           0.           1.           0.           1.\n",
            " 100.           1.85203897   4.70268377   4.86451624  85.30507892\n",
            "  46.37965915]\n",
            "local_CPU 0\n",
            "\n",
            "reward -8.874952645068502\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.85203897   4.70268377   4.86451624  85.30507892\n",
            "  46.37965915]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.6151, 10.9062,\n",
            "         11.4848, 66.4186, 22.9983]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.8520,\n",
            "           4.7027,   4.8645,  85.3051,  46.3797]])\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.61512294 10.90622176 11.48480387 66.41861702 22.99832933]\n",
            "local_CPU 0\n",
            "reward -11.287567313708129\n",
            "action 4 w00\n",
            "w01 episode  1986 reward -11.3\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.85203897   4.70268377   4.86451624  85.30507892\n",
            "  46.37965915]\n",
            "channels [3.6151229417021344, 10.906221759647124, 11.48480387362076, 66.41861702236261, 22.998329330938294]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           5.63009481   5.01012645   7.7552564   17.33250743\n",
            "  16.22688787]local_CPU 0\n",
            "reward -2.1680430213496775\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]\n",
            "Task 80.0\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.63009481   5.01012645   7.7552564   17.33250743\n",
            "  16.22688787]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   5.6301,\n",
            "           5.0101,   7.7553,  17.3325,  16.2269]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.5467,  9.4858,\n",
            "          3.3309, 25.3735, 58.2250]])\n",
            "\n",
            "action 4 w01\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           5.63009481   5.01012645   7.7552564   17.33250743\n",
            "  16.22688787]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -2.435381690010505\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "reward -4.253776754590586\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]\n",
            "Task 80.0\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.8288,  3.7668,\n",
            "          2.0465, 28.9770, 27.3834]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.6137,  4.7303,\n",
            "          6.5490, 70.5084, 46.8251]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.82880039  3.766786    2.04651886 28.97697957 27.3833544 ]\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]local_CPU 0\n",
            "\n",
            "reward -4.7167740203725845\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.85203897  4.70268377  4.86451624 85.30507892 46.37965915]local_CPU 0\n",
            "reward -6.431730529815866\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.24812264  5.55829247  8.51740158 35.05376762 16.09647356]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.85203897  4.70268377  4.86451624 85.30507892 46.37965915]Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.24812264  5.55829247  8.51740158 35.05376762 16.09647356]\n",
            "\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.2481,  5.5583,\n",
            "          8.5174, 35.0538, 16.0965]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.8520,  4.7027,\n",
            "          4.8645, 85.3051, 46.3797]])\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.24812264  5.55829247  8.51740158 35.05376762 16.09647356]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.85203897  4.70268377  4.86451624 85.30507892 46.37965915]\n",
            "local_CPU 0\n",
            "\n",
            "local_CPU 0\n",
            "reward -9.335975067975479\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]reward -6.800911873734094\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]Task 20.0\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.9549,  5.2856,\n",
            "         10.1484, 40.4489, 38.8182]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.5467,  9.4858,\n",
            "          3.3309, 25.3735, 58.2250]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.54665871  9.48576544  3.33088617 25.37352994 58.22499481]\n",
            "local_CPU 0\n",
            "reward -11.549692384195819\n",
            "\n",
            "w00 episode  1987 reward -11.5\n",
            "channels [2.9548635173923694, 5.285635019843205, 10.14837497137985, 40.4489006755077, 38.81821503054366]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.76631937   4.43051783   5.34228812  10.78477209\n",
            "  40.8147626 ]\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "reward -8.809135630265686\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.76631937   4.43051783   5.34228812  10.78477209\n",
            "  40.8147626 ]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.7663,\n",
            "           4.4305,   5.3423,  10.7848,  40.8148]])\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.76631937   4.43051783   5.34228812  10.78477209\n",
            "  40.8147626 ][ 1.          0.          1.          0.          1.         20.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]\n",
            "local_CPU 0\n",
            "reward -2.2140486347438086\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]\n",
            "Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.9795,  4.2953,\n",
            "          5.5405,  9.3690, 50.9006]])\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.6137,  4.7303,\n",
            "          6.5490, 70.5084, 46.8251]])action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.61368625  4.7302572   6.54903277 70.50844486 46.82506001]local_CPU 0\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.3323255708744375\n",
            "reward -10.908862561238156\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]w01 episode  1988 reward -10.9\n",
            "\n",
            "channels [0.6136862467102214, 4.730257204650689, 6.549032767984052, 70.50844485947822, 46.825060014268026]\n",
            "Task 60.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.24812264   5.55829247   8.51740158  35.05376762\n",
            "  16.09647356]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.4257,  5.6487,\n",
            "          4.1950, 27.0027, 44.7405]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.24812264   5.55829247   8.51740158  35.05376762\n",
            "  16.09647356]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.2481,\n",
            "           5.5583,   8.5174,  35.0538,  16.0965]])action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]action 4 w01\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.24812264   5.55829247   8.51740158  35.05376762\n",
            "  16.09647356]local_CPU 0\n",
            "\n",
            "reward -6.490062220679657\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.70864202  9.70021186  2.41101456 34.36566043 28.51516017]local_CPU 0\n",
            "\n",
            "Task 40.0\n",
            "reward -2.5639957699505675\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.70864202  9.70021186  2.41101456 34.36566043 28.51516017]\n",
            "\n",
            "Task 80.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.7086,  9.7002,\n",
            "          2.4110, 34.3657, 28.5152]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]\n",
            "\n",
            "action 4 w00\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.9549,  5.2856,\n",
            "         10.1484, 40.4489, 38.8182]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.70864202  9.70021186  2.41101456 34.36566043 28.51516017]\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.95486352  5.28563502 10.14837497 40.44890068 38.81821503]local_CPU 0\n",
            "reward -8.836943948048951\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392]local_CPU 0\n",
            "\n",
            "Task 20.0\n",
            "reward -4.669086799601474\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392][ 1.          0.          1.          0.          1.         60.\n",
            "  2.76631937  4.43051783  5.34228812 10.78477209 40.8147626 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.7463,  9.0893,\n",
            "          2.1303, 61.9723, 32.0139]])Task 60.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.76631937  4.43051783  5.34228812 10.78477209 40.8147626 ]action 4 w00\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.7663,  4.4305,\n",
            "          5.3423, 10.7848, 40.8148]])observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392]\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -11.177530075314289\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.76631937  4.43051783  5.34228812 10.78477209 40.8147626 ]w00 episode  1989 reward -11.2\n",
            "channels [2.7463091533085575, 9.089333825369986, 2.130299940188809, 61.972279605333874, 32.01389391722808]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           4.46448038   2.42149413   5.80932544  18.469126\n",
            "  65.71146079]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "reward -6.787815051106222\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.46448038   2.42149413   5.80932544  18.469126\n",
            "  65.71146079]\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   4.4645,\n",
            "           2.4215,   5.8093,  18.4691,  65.7115]])\n",
            "\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           4.46448038   2.42149413   5.80932544  18.469126\n",
            "  65.71146079]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]\n",
            "local_CPU 0\n",
            "reward -2.0597822280942424\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.8329,  3.3121,\n",
            "          4.5182, 12.5081, 34.0987]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.365907568347669\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]\n",
            "Task 60.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.9795,  4.2953,\n",
            "          5.5405,  9.3690, 50.9006]])\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]action 4 w01\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.97945803  4.29527557  5.54047897  9.36899247 50.90062694]\n",
            "reward -8.818102665958563\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]\n",
            "Task 20.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  0.3953, 11.6947,\n",
            "          9.0070, 30.9152, 26.3485]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]\n",
            "\n",
            "local_CPU 0\n",
            "reward -6.779601003790416\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.4257,  5.6487,\n",
            "          4.1950, 27.0027, 44.7405]])[ 1.          0.          1.          0.          1.         40.\n",
            "  2.07748055  3.4968303   9.1495668  26.75088277 53.57929103]\n",
            "action 4 w01\n",
            "\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.42571823  5.64867213  4.19500826 27.00272351 44.74045991]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.07748055  3.4968303   9.1495668  26.75088277 53.57929103]\n",
            "\n",
            "local_CPU 0\n",
            "reward -10.878130418985624\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.0775,  3.4968,\n",
            "          9.1496, 26.7509, 53.5793]])w01 episode  1990 reward -10.9\n",
            "channels [1.425718231278965, 5.648672128921179, 4.1950082587704, 27.0027235134304, 44.740459914476]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           1.70864202   9.70021186   2.41101456  34.36566043\n",
            "  28.51516017]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.70864202   9.70021186   2.41101456  34.36566043\n",
            "  28.51516017]\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.7086,\n",
            "           9.7002,   2.4110,  34.3657,  28.5152]])\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.70864202   9.70021186   2.41101456  34.36566043\n",
            "  28.51516017]\n",
            "local_CPU 0\n",
            "reward -2.1893184503707235\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.07748055  3.4968303   9.1495668  26.75088277 53.57929103][ 1.          0.          1.          0.          1.         80.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392]\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.7463,  9.0893,\n",
            "          2.1303, 61.9723, 32.0139]])reward -8.911753451262925\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.74630915  9.08933383  2.13029994 61.97227961 32.01389392]Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]local_CPU 0\n",
            "\n",
            "reward -4.409222534425622\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  4.46448038  2.42149413  5.80932544 18.469126   65.71146079]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.5861,  8.0971,\n",
            "          3.7022, 33.3156, 60.8197]])\n",
            "\n",
            "Task 60.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.46448038  2.42149413  5.80932544 18.469126   65.71146079]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]\n",
            "\n",
            "local_CPU 0\n",
            "reward -11.00451815954868\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  4.4645,  2.4215,\n",
            "          5.8093, 18.4691, 65.7115]])w00 episode  1991 reward -11.0\n",
            "\n",
            "channels [2.5861462324889595, 8.097127730601162, 3.7021537096264194, 33.31564665194936, 60.81966238997887]\n",
            "action 4 w01\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.1307632   11.90996489   7.15090023  43.72585881\n",
            "  35.74393809]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  4.46448038  2.42149413  5.80932544 18.469126   65.71146079]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.1307632   11.90996489   7.15090023  43.72585881\n",
            "  35.74393809]reward -6.399464598369447\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.1308,\n",
            "          11.9100,   7.1509,  43.7259,  35.7439]])\n",
            "\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.1307632   11.90996489   7.15090023  43.72585881\n",
            "  35.74393809]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.8329,  3.3121,\n",
            "          4.5182, 12.5081, 34.0987]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.83290533  3.31211818  4.51819069 12.50811609 34.09870969]\n",
            "local_CPU 0\n",
            "reward -2.2743358216076888\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147]local_CPU 0\n",
            "\n",
            "reward -8.593474097277578\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]\n",
            "Task 20.0\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  0.3953, 11.6947,\n",
            "          9.0070, 30.9152, 26.3485]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.8977, 13.5144,\n",
            "          5.0250, 27.7490, 19.4554]])\n",
            "action 4 w01\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  0.39534622 11.69470589  9.00699654 30.91521075 26.34847942]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -10.840001864664831\n",
            "reward -4.9007328621757225\n",
            "w01 episode  1992 reward -10.8\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217]channels [0.3953462226948133, 11.694705894106901, 9.006996541821186, 30.915210752833303, 26.348479418838757]\n",
            "\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.07748055   3.4968303    9.1495668   26.75088277\n",
            "  53.57929103]\n",
            "Task 60.0\n",
            "Task 100.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.07748055   3.4968303    9.1495668   26.75088277\n",
            "  53.57929103]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.0102,  5.7202,\n",
            "          7.2846, 17.8818, 17.1749]])state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.0775,\n",
            "           3.4968,   9.1496,  26.7509,  53.5793]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.07748055   3.4968303    9.1495668   26.75088277\n",
            "  53.57929103]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -7.849132092040627\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.76572056  5.68346227  6.53410681 36.47668463 41.46358599]reward -2.062342569411887\n",
            "\n",
            "Task 40.0\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.76572056  5.68346227  6.53410681 36.47668463 41.46358599]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.7657,  5.6835,\n",
            "          6.5341, 36.4767, 41.4636]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.76572056  5.68346227  6.53410681 36.47668463 41.46358599]\n",
            "Task 80.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]reward -10.059981351925847\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  2.5861,  8.0971,\n",
            "          3.7022, 33.3156, 60.8197]])\n",
            "Task 20.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394]action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  2.58614623  8.09712773  3.70215371 33.31564665 60.81966239]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.1741,  8.7576,\n",
            "          5.5053, 26.7511, 11.0748]])\n",
            "\n",
            "local_CPU 0\n",
            "reward -4.090492346669321\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.1307632  11.90996489  7.15090023 43.72585881 35.74393809]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394]\n",
            "\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.1307632  11.90996489  7.15090023 43.72585881 35.74393809]local_CPU 0\n",
            "reward -12.349026845119587\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.1308, 11.9100,\n",
            "          7.1509, 43.7259, 35.7439]])w00 episode  1993 reward -12.3\n",
            "\n",
            "channels [3.1740876855844893, 8.757589716707805, 5.505319467981167, 26.751122571977383, 11.074813939776583]\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.1307632  11.90996489  7.15090023 43.72585881 35.74393809][  1.           0.           1.           0.           1.\n",
            " 100.           1.12834915   1.89336714   6.21186298  31.08462985\n",
            "  13.09755203]\n",
            "\n",
            "local_CPU 0\n",
            "Task 100.0\n",
            "reward -6.256186595905163\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.12834915   1.89336714   6.21186298  31.08462985\n",
            "  13.09755203][ 1.          0.          1.          0.          1.         40.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   1.1283,\n",
            "           1.8934,   6.2119,  31.0846,  13.0976]])Task 40.0\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147]action 4 w00\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           1.12834915   1.89336714   6.21186298  31.08462985\n",
            "  13.09755203]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.8977, 13.5144,\n",
            "          5.0250, 27.7490, 19.4554]])\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w01\n",
            "reward -3.198260822795278\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.89771755 13.51438593  5.02497197 27.74898142 19.45538147][ 1.          0.          1.          0.          1.         80.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528]\n",
            "\n",
            "local_CPU 0\n",
            "Task 80.0\n",
            "reward -8.620093795237025\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528][ 1.          0.          1.          0.          1.         20.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.8996,  5.7080,\n",
            "         13.4436, 36.9403, 31.0431]])\n",
            "\n",
            "Task 20.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.0102,  5.7202,\n",
            "          7.2846, 17.8818, 17.1749]])\n",
            "local_CPU 0\n",
            "\n",
            "reward -5.507820155326872\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.01018353  5.72020606  7.28460335 17.88176627 17.17488217][ 1.          0.          1.          0.          1.         60.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]\n",
            "\n",
            "local_CPU 0\n",
            "Task 60.0\n",
            "reward -11.27693514148597\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]w01 episode  1994 reward -11.3\n",
            "\n",
            "channels [2.0101835303404707, 5.720206060907384, 7.284603351973507, 17.881766266077136, 17.174882165136335]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.9658, 13.3591,\n",
            "          5.5243, 45.6293, 36.5129]])[  1.           0.           1.           0.           1.\n",
            " 100.           0.76572056   5.68346227   6.53410681  36.47668463\n",
            "  41.46358599]\n",
            "\n",
            "Task 100.0\n",
            "action 4 w00\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.76572056   5.68346227   6.53410681  36.47668463\n",
            "  41.46358599]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   0.7657,\n",
            "           5.6835,   6.5341,  36.4767,  41.4636]])local_CPU 0\n",
            "\n",
            "reward -7.7416745216447875\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.10840789  7.45130241  8.41262186 36.6204322  39.43102271]observation [  1.           0.           1.           0.           1.\n",
            " 100.           0.76572056   5.68346227   6.53410681  36.47668463\n",
            "  41.46358599]\n",
            "Task 40.0\n",
            "\n",
            "local_CPU 0\n",
            "reward -2.1181085261322274\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.10840789  7.45130241  8.41262186 36.6204322  39.43102271]\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.1084,  7.4513,\n",
            "          8.4126, 36.6204, 39.4310]])\n",
            "Task 80.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.10840789  7.45130241  8.41262186 36.6204322  39.43102271]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.1741,  8.7576,\n",
            "          5.5053, 26.7511, 11.0748]])local_CPU 0\n",
            "\n",
            "action 4 w01\n",
            "reward -9.98988889444434\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.17408769  8.75758972  5.50531947 26.75112257 11.07481394][ 1.          0.          1.          0.          1.         20.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]\n",
            "\n",
            "local_CPU 0\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]reward -5.4480798432042095\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.12834915  1.89336714  6.21186298 31.08462985 13.09755203]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  4.3676, 21.0771,\n",
            "          5.1398, 10.5428, 20.5344]])\n",
            "\n",
            "action 4 w00\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.12834915  1.89336714  6.21186298 31.08462985 13.09755203]\n",
            "local_CPU 0\n",
            "\n",
            "reward -12.522804573404496\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.1283,  1.8934,\n",
            "          6.2119, 31.0846, 13.0976]])w00 episode  1995 reward -12.5\n",
            "\n",
            "channels [4.3675863616939665, 21.077079415162505, 5.139796267057939, 10.542824918465568, 20.534386361371592]\n",
            "action 4 w01\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.03086166   4.51718168   2.39851437   9.77696511\n",
            "  23.28466473]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.12834915  1.89336714  6.21186298 31.08462985 13.09755203]\n",
            "\n",
            "Task 100.0\n",
            "local_CPU 0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.03086166   4.51718168   2.39851437   9.77696511\n",
            "  23.28466473]reward -8.159273679912058\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528]state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.0309,\n",
            "           4.5172,   2.3985,   9.7770,  23.2847]])\n",
            "\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.03086166   4.51718168   2.39851437   9.77696511\n",
            "  23.28466473]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.8996,  5.7080,\n",
            "         13.4436, 36.9403, 31.0431]])local_CPU 0\n",
            "\n",
            "reward -2.539621564096584\n",
            "action 4 w01\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.89955917  5.70801058 13.44359569 36.94026494 31.04314528]\n",
            "\n",
            "Task 80.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]reward -10.329005072210007\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.9565,  4.2752,\n",
            "          4.4332, 36.6216, 17.0925]])[ 1.          0.          1.          0.          1.         20.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]\n",
            "\n",
            "action 4 w00\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]\n",
            "\n",
            "local_CPU 0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.9658, 13.3591,\n",
            "          5.5243, 45.6293, 36.5129]])reward -5.280443652620988\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.96576571 13.35908339  5.52430499 45.62934249 36.51294426]\n",
            "\n",
            "Task 60.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]reward -12.445186316795919\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.3304, 12.3617,\n",
            "          5.0149, 63.9676, 30.2989]])w01 episode  1996 reward -12.4\n",
            "\n",
            "action 4 w00\n",
            "channels [1.9657657073734347, 13.359083386563766, 5.524304987148764, 45.629342485286315, 36.512944255643895]\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           2.10840789   7.45130241   8.41262186  36.6204322\n",
            "  39.43102271]\n",
            "local_CPU 0\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.10840789   7.45130241   8.41262186  36.6204322\n",
            "  39.43102271]reward -7.618400816506462\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  3.35207445  2.95264399 13.00681991 22.17346784  3.32235032]\n",
            "Task 40.0\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   2.1084,\n",
            "           7.4513,   8.4126,  36.6204,  39.4310]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.35207445  2.95264399 13.00681991 22.17346784  3.32235032]observation [  1.           0.           1.           0.           1.\n",
            " 100.           2.10840789   7.45130241   8.41262186  36.6204322\n",
            "  39.43102271]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  3.3521,  2.9526,\n",
            "         13.0068, 22.1735,  3.3224]])\n",
            "local_CPU 0\n",
            "reward -2.155791184435926\n",
            "\n",
            "action 4 w00\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]observation [ 1.          0.          1.          0.          1.         40.\n",
            "  3.35207445  2.95264399 13.00681991 22.17346784  3.32235032]\n",
            "local_CPU 0\n",
            "\n",
            "reward -10.264376227078994\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]Task 80.0\n",
            "\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  4.3676, 21.0771,\n",
            "          5.1398, 10.5428, 20.5344]])\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  3.5147, 13.6966,\n",
            "          4.0562, 30.2732, 41.2277]])\n",
            "action 4 w01\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  4.36758636 21.07707942  5.13979627 10.54282492 20.53438636]action 4 w00\n",
            "\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -12.483002268573179\n",
            "reward -4.432920319383023\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  2.03086166  4.51718168  2.39851437  9.77696511 23.28466473]\n",
            "w00 episode  1997 reward -12.5\n",
            "channels [3.5146939080368336, 13.696620514049702, 4.056151210663627, 30.273151987801782, 41.22767556887961]\n",
            "Task 60.0\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.12005349   8.75273732  12.62011203  23.08008964\n",
            "  81.91983864]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.03086166  4.51718168  2.39851437  9.77696511 23.28466473]\n",
            "\n",
            "Task 100.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  2.0309,  4.5172,\n",
            "          2.3985,  9.7770, 23.2847]])\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.12005349   8.75273732  12.62011203  23.08008964\n",
            "  81.91983864]\n",
            "action 4 w01\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.1201,\n",
            "           8.7527,  12.6201,  23.0801,  81.9198]])\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  2.03086166  4.51718168  2.39851437  9.77696511 23.28466473]\n",
            "local_CPU 0\n",
            "reward -6.78370106918269\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]\n",
            "Task 40.0\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.12005349   8.75273732  12.62011203  23.08008964\n",
            "  81.91983864]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  0.9565,  4.2752,\n",
            "          4.4332, 36.6216, 17.0925]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  0.95650517  4.27524611  4.43323348 36.62162448 17.09249415]\n",
            "local_CPU 0\n",
            "\n",
            "reward -9.200730789819723\n",
            "local_CPU 0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]reward -2.027815949305745\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507]Task 20.0\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  1.4429,  1.4828,\n",
            "          8.1408,  7.5278, 37.3160]])\n",
            "\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507]state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.3304, 12.3617,\n",
            "          5.0149, 63.9676, 30.2989]])\n",
            "\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.33040895 12.3617007   5.01492543 63.96757736 30.29892289]\n",
            "local_CPU 0\n",
            "reward -11.400261529782904\n",
            "local_CPU 0\n",
            "w01 episode  1998 reward -11.4\n",
            "channels [2.3304089535654993, 12.36170069718961, 5.0149254267064025, 63.96757735843891, 30.29892288677616]\n",
            "[  1.           0.           1.           0.           1.\n",
            " 100.           3.35207445   2.95264399  13.00681991  22.17346784\n",
            "   3.32235032]\n",
            "Task 100.0\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.35207445   2.95264399  13.00681991  22.17346784\n",
            "   3.32235032]reward -4.276659516589639\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]\n",
            "Task 60.0\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.3521,\n",
            "           2.9526,  13.0068,  22.1735,   3.3224]])observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  1.7602,  1.8426,\n",
            "          8.8892, 10.3145, 45.4765]])observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.35207445   2.95264399  13.00681991  22.17346784\n",
            "   3.32235032]\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]reward -2.5075489866502414\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]local_CPU 0\n",
            "\n",
            "reward -6.4257720925124\n",
            "Task 80.0\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  4.25348324  6.02196865 11.56507035 10.24466632 60.35877115]observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]\n",
            "\n",
            "Task 40.0\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  3.5147, 13.6966,\n",
            "          4.0562, 30.2732, 41.2277]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.25348324  6.02196865 11.56507035 10.24466632 60.35877115]\n",
            "\n",
            "action 4 w01\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  4.2535,  6.0220,\n",
            "         11.5651, 10.2447, 60.3588]])observation [ 1.          0.          1.          0.          1.         80.\n",
            "  3.51469391 13.69662051  4.05615121 30.27315199 41.22767557]\n",
            "\n",
            "action 4 w00\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  4.25348324  6.02196865 11.56507035 10.24466632 60.35877115]reward -4.634791119412276\n",
            "\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  3.12005349  8.75273732 12.62011203 23.08008964 81.91983864]local_CPU 0\n",
            "\n",
            "reward -8.526729001291582\n",
            "Task 60.0\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  5.73317186  6.30369855  5.29911284 52.164342   20.99664266]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.12005349  8.75273732 12.62011203 23.08008964 81.91983864]observation [ 1.          0.          1.          0.          1.         20.\n",
            "  5.73317186  6.30369855  5.29911284 52.164342   20.99664266]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  5.7332,  6.3037,\n",
            "          5.2991, 52.1643, 20.9966]])state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  3.1201,  8.7527,\n",
            "         12.6201, 23.0801, 81.9198]])\n",
            "\n",
            "action 4 w00\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  5.73317186  6.30369855  5.29911284 52.164342   20.99664266]observation [ 1.          0.          1.          0.          1.         60.\n",
            "  3.12005349  8.75273732 12.62011203 23.08008964 81.91983864]\n",
            "\n",
            "local_CPU 0\n",
            "local_CPU 0\n",
            "reward -6.61093769619858\n",
            "reward -11.068659113346754\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507]w00 episode  1999 reward -11.1\n",
            "\n",
            "Task 40.0\n",
            "channels [5.733171861372275, 6.30369854597388, 5.299112838408777, 52.164341996263666, 20.99664266275742]\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507][  1.           0.           1.           0.           1.\n",
            " 100.           3.85377459  10.51004587  10.57904778  19.80399719\n",
            "  20.4805356 ]\n",
            "\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  1.4429,  1.4828,\n",
            "          8.1408,  7.5278, 37.3160]])Task 100.0\n",
            "\n",
            "observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.85377459  10.51004587  10.57904778  19.80399719\n",
            "  20.4805356 ]action 4 w01\n",
            "\n",
            "state tensor([[  1.0000,   0.0000,   1.0000,   0.0000,   1.0000, 100.0000,   3.8538,\n",
            "          10.5100,  10.5790,  19.8040,  20.4805]])observation [ 1.          0.          1.          0.          1.         40.\n",
            "  1.44289081  1.48278192  8.1408288   7.52778062 37.31604507]\n",
            "\n",
            "local_CPU 0\n",
            "action 4 w00\n",
            "reward -8.754241766120991\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]observation [  1.           0.           1.           0.           1.\n",
            " 100.           3.85377459  10.51004587  10.57904778  19.80399719\n",
            "  20.4805356 ]\n",
            "\n",
            "Task 20.0\n",
            "local_CPU 0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  1.7602,  1.8426,\n",
            "          8.8892, 10.3145, 45.4765]])\n",
            "action 4 w01\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  1.7601992   1.84262305  8.88917011 10.31449606 45.4764744 ]\n",
            "local_CPU 0\n",
            "reward -10.804967715526484\n",
            "reward -2.66914070922811\n",
            "w01 episode  2000 reward -10.8\n",
            "[ 1.          0.          1.          0.          1.         80.\n",
            "  0.39138618 12.49821354  5.07889302 14.03017989 71.60758115]channels [1.7601992031978158, 1.8426230529445424, 8.889170114484836, 10.31449606345619, 45.47647440168656]\n",
            "\n",
            "Task 80.0\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.39138618 12.49821354  5.07889302 14.03017989 71.60758115]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 80.0000,  0.3914, 12.4982,\n",
            "          5.0789, 14.0302, 71.6076]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         80.\n",
            "  0.39138618 12.49821354  5.07889302 14.03017989 71.60758115]\n",
            "local_CPU 0\n",
            "reward -4.754012048558643\n",
            "[ 1.          0.          1.          0.          1.         60.\n",
            "  6.01365132  9.80914195  6.37901903 19.45275209 56.58660084]\n",
            "Task 60.0\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  6.01365132  9.80914195  6.37901903 19.45275209 56.58660084]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 60.0000,  6.0137,  9.8091,\n",
            "          6.3790, 19.4528, 56.5866]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         60.\n",
            "  6.01365132  9.80914195  6.37901903 19.45275209 56.58660084]\n",
            "local_CPU 0\n",
            "reward -6.86648750066641\n",
            "[ 1.          0.          1.          0.          1.         40.\n",
            "  2.9892855   8.21630029  4.61766124 38.93478187 24.33835157]\n",
            "Task 40.0\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.9892855   8.21630029  4.61766124 38.93478187 24.33835157]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 40.0000,  2.9893,  8.2163,\n",
            "          4.6177, 38.9348, 24.3384]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         40.\n",
            "  2.9892855   8.21630029  4.61766124 38.93478187 24.33835157]\n",
            "local_CPU 0\n",
            "reward -9.383804415720192\n",
            "[ 1.          0.          1.          0.          1.         20.\n",
            "  2.20588811  8.80326924  5.76565763 18.35591465 24.77313466]\n",
            "Task 20.0\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.20588811  8.80326924  5.76565763 18.35591465 24.77313466]\n",
            "state tensor([[ 1.0000,  0.0000,  1.0000,  0.0000,  1.0000, 20.0000,  2.2059,  8.8033,\n",
            "          5.7657, 18.3559, 24.7731]])\n",
            "action 4 w00\n",
            "observation [ 1.          0.          1.          0.          1.         20.\n",
            "  2.20588811  8.80326924  5.76565763 18.35591465 24.77313466]\n",
            "local_CPU 0\n",
            "reward -11.873932887999949\n",
            "w00 episode  2001 reward -11.9\n",
            "channels [2.205888110997172, 8.803269243711808, 5.765657629784757, 18.35591465136956, 24.773134656133013]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import torch as T\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "from math import log\n",
        "import math\n",
        "\n",
        "class SharedAdam(T.optim.Adam):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
        "            weight_decay=0):\n",
        "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,\n",
        "                weight_decay=weight_decay)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['step'] = 0\n",
        "                state['exp_avg'] = T.zeros_like(p.data)\n",
        "                state['exp_avg_sq'] = T.zeros_like(p.data)\n",
        "\n",
        "                state['exp_avg'].share_memory_()\n",
        "                state['exp_avg_sq'].share_memory_()\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dims, n_actions, gamma=0.99):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.pi1 = nn.Linear(*input_dims, 128)\n",
        "        self.v1 = nn.Linear(*input_dims, 128)\n",
        "        self.pi = nn.Linear(128, n_actions)\n",
        "        self.v = nn.Linear(128, 1)\n",
        "\n",
        "        self.rewards = []\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "\n",
        "    def remember(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, state):\n",
        "        pi1 = F.relu(self.pi1(state))\n",
        "        v1 = F.relu(self.v1(state))\n",
        "\n",
        "        pi = self.pi(pi1)\n",
        "        v = self.v(v1)\n",
        "\n",
        "        return pi, v\n",
        "\n",
        "    def calc_R(self, done):\n",
        "        states = T.tensor(self.states, dtype=T.float)\n",
        "        _, v = self.forward(states)\n",
        "\n",
        "        R = v[-1]*(1-int(done))\n",
        "\n",
        "        batch_return = []\n",
        "        for reward in self.rewards[::-1]:\n",
        "            R = reward + self.gamma*R\n",
        "            batch_return.append(R)\n",
        "        batch_return.reverse()\n",
        "        batch_return = T.tensor(batch_return, dtype=T.float)\n",
        "\n",
        "        return batch_return\n",
        "\n",
        "    def calc_loss(self, done):\n",
        "        states = T.tensor(self.states, dtype=T.float)\n",
        "        actions = T.tensor(self.actions, dtype=T.float)\n",
        "\n",
        "        returns = self.calc_R(done)\n",
        "\n",
        "        pi, values = self.forward(states)\n",
        "        values = values.squeeze()\n",
        "        critic_loss = (returns-values)**2\n",
        "\n",
        "        probs = T.softmax(pi, dim=1)\n",
        "        dist = Categorical(probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        actor_loss = -log_probs*(returns-values)\n",
        "\n",
        "        total_loss = (critic_loss + actor_loss).mean()\n",
        "    \n",
        "        return total_loss\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float)\n",
        "        print('state',state)\n",
        "        pi, v = self.forward(state)\n",
        "        probs = T.softmax(pi, dim=1)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample().numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "class Agent(mp.Process):\n",
        "    def __init__(self, global_actor_critic, optimizer, input_dims, n_actions, \n",
        "                gamma, lr, name, global_ep_idx, env_id,envi):\n",
        "        super(Agent, self).__init__()\n",
        "        self.local_actor_critic = ActorCritic(input_dims, n_actions, gamma)\n",
        "        self.global_actor_critic = global_actor_critic\n",
        "        self.name = 'w%02i' % name\n",
        "        self.episode_idx = global_ep_idx\n",
        "        self.env = MecSvrEnv(envi[0], envi[1],envi[2])\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def run(self):\n",
        "        reward_history = []\n",
        "        #channels = self.env.get_channels()\n",
        "        actions = []\n",
        "        t_step = 1\n",
        "        flag =False\n",
        "        while self.episode_idx.value < N_GAMES:\n",
        "            done = False       \n",
        "            #offloading_matrix =  np.random.randint(2, size=(5))\n",
        "            offloading_matrix = [1,0,1,0,1]\n",
        "            rewards = np.zeros(5)\n",
        "            local_CPU = 0\n",
        "            reward = 0\n",
        "            local_reward = 0\n",
        "            reward_server =0\n",
        "            reward_servers = 0\n",
        "            Task = 100\n",
        "            self.local_actor_critic.clear_memory()\n",
        "            while Task:\n",
        "              #while observation[5]:\n",
        "                channels = self.env.get_channels()\n",
        "                observation = self.env.reset(isTrain, Task,channels, offloading_matrix)\n",
        "                Task = observation[5]\n",
        "                print('Task', Task)\n",
        "                print('observation', observation)\n",
        "                #observation = self.env.reset(isTrain)\n",
        "                #observation[5] = Task        \n",
        "                action = self.local_actor_critic.choose_action(observation)   \n",
        "                actions.append(action)     \n",
        "                print('action',action, self.name)\n",
        "                print('observation', observation)\n",
        "                observation_, transmission, computing_server, Task, local_CPU, feedback , done, offloading_server, info  = self.env.step(action,observation, channels)\n",
        "                print('local_CPU', local_CPU)\n",
        "                reward, flag = self.env.calcul_reward(offloading_server,transmission, computing_server, local_CPU, feedback, done)\n",
        "                if flag == False :\n",
        "                  local_reward = local_reward + reward\n",
        "                  print('local_reward', local_reward)\n",
        "                  #reward_action = local_reward\n",
        "                if flag == True:\n",
        "                  #reward_server = reward_server + reward\n",
        "                  #rewards[action] = rewards[action] + reward\n",
        "                  reward_server = reward_server + reward\n",
        "                  print('reward', reward_server)\n",
        "                  #reward_action = rewards[action]\n",
        "                  #reward_action = reward_server\n",
        "                self.local_actor_critic.remember(observation, action, reward)\n",
        "                if t_step % T_MAX == 0 or done:\n",
        "                    loss = self.local_actor_critic.calc_loss(done)\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    for local_param, global_param in zip(\n",
        "                            self.local_actor_critic.parameters(),\n",
        "                            self.global_actor_critic.parameters()):\n",
        "                        global_param._grad = local_param.grad\n",
        "                    self.optimizer.step()\n",
        "                    self.local_actor_critic.load_state_dict(\n",
        "                            self.global_actor_critic.state_dict())\n",
        "                    self.local_actor_critic.clear_memory()\n",
        "                t_step += 1\n",
        "                observation = observation_\n",
        "            if len(rewards)!=0:\n",
        "              #reward_server = min(rewards)\n",
        "              reward_total = min(reward_server,local_reward)\n",
        "              #reward_total = reward_server + local_reward\n",
        "            else:\n",
        "              reward_total =  local_reward\n",
        "            if reward_total < -50:\n",
        "              reward_total = -50\n",
        "            with self.episode_idx.get_lock():\n",
        "                self.episode_idx.value += 1\n",
        "            print(self.name, 'episode ', self.episode_idx.value, 'reward %.1f' % reward_total)\n",
        "            reward_history.append(reward_total)\n",
        "            print('channels',channels)\n",
        "        savetxt('reward_'+self.name+'.csv', reward_history, delimiter=',')\n",
        "        savetxt('actions'+self.name+'.csv', actions, delimiter=',')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    t_factor = 0.3\n",
        "    isTrain = False\n",
        "    server_config = [{'id':'1', 'rate':3.0, 'dis':100, \n",
        "                        't_factor':t_factor, 'penalty':1000, 'F_e':400},\n",
        "                 {'id':'2', 'rate':3.0, 'dis':150, \n",
        "                        't_factor':t_factor, 'penalty':1000, 'F_e':500},\n",
        "                 {'id':'3', 'rate':3.0, 'dis':150, \n",
        "                       't_factor':t_factor, 'penalty':1000, 'F_e':600},\n",
        "                 {'id':'4', 'rate':3.0, 'dis':150, \n",
        "                       't_factor':t_factor, 'penalty':1000, 'F_e':500},\n",
        "                 {'id':'5', 'rate':3.0, 'dis':100, \n",
        "                       't_factor':t_factor, 'penalty':1000, 'F_e':300}\n",
        "                ]\n",
        "#parmetre de train\n",
        "    train_config = {'minibatch_size':64, 'tau':0.001,\n",
        "                        'gamma':0.99, 'buffer_size':250000, 'learning_rate':0.0001, 'sigma2': 0.2}  #0.2 dbm\n",
        "    user_config = [{'id':'0', 'rate':3.0, 'dis':50, \n",
        "                            't_factor':t_factor, 'penalty':1000, 'M_u':100, 'P_u':8, 'F_u':200},\n",
        "               {'id':'1', 'rate':3.0, 'dis':50, \n",
        "                            't_factor':t_factor, 'penalty':1000, 'M_u':100, 'P_u':8, 'F_u':200}\n",
        "               ]\n",
        "    #initialize all servers\n",
        "    server_list = []\n",
        "    for info in server_config:\n",
        "        server_list.append(Server_initialization(info, train_config))\n",
        "        print('Initialization OK!----> server ' + info['id'])\n",
        "    print(len(server_list))\n",
        "\n",
        "    user_list = []\n",
        "    for info in user_config:\n",
        "        user_list.append(User_initialization(info, train_config))\n",
        "        print('Initialization OK!----> user ' + info['id'])\n",
        "    print(user_list)\n",
        "    lr = 1e-3\n",
        "    envi = [user_list, server_list,train_config['sigma2']]\n",
        "    env_id = 'CartPole-v0'\n",
        "    n_actions = 5\n",
        "    input_dims = [11]\n",
        "    N_GAMES = 2000\n",
        "    T_MAX = 5\n",
        "    global_actor_critic = ActorCritic(input_dims, n_actions)\n",
        "    global_actor_critic.share_memory()\n",
        "    optim = SharedAdam(global_actor_critic.parameters(), lr=lr, \n",
        "                        betas=(0.92, 0.999))\n",
        "    global_ep = mp.Value('i', 0)\n",
        "\n",
        "    workers = [Agent(global_actor_critic,\n",
        "                    optim,\n",
        "                    input_dims,\n",
        "                    n_actions,\n",
        "                    gamma=0.99,\n",
        "                    lr=lr,\n",
        "                    name=i,\n",
        "                    global_ep_idx=global_ep,\n",
        "                    env_id=env_id,\n",
        "                    envi=envi) for i in range(2)]\n",
        "    [w.start() for w in workers]\n",
        "    [w.join() for w in workers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GA5OHN9NrvyI"
      },
      "outputs": [],
      "source": [
        "data_w00 = loadtxt('/content/reward_w00.csv', delimiter=',')\n",
        "data_w01 = loadtxt('/content/reward_w01.csv', delimiter=',')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_learning_curve(x, data_w00,w, figure_file):\n",
        "    running_avg_w00 = np.zeros(len(data_w00))\n",
        "    for i in range(len(data_w00)):\n",
        "        running_avg_w00[i] = np.mean(data_w00[max(0, i-len(data_w00)):(i+1)])\n",
        "    running_avg_w00= running_avg_w00[0:len(data_w00)]\n",
        "    plt.plot(x, data_w00, label= w+'_Reward')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Average Rewards\")\n",
        "    plt.title(' ')\n",
        "    plt.savefig(figure_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hKllRb0TqnAi",
        "outputId": "c96e3e9f-5fac-4df8-9124-adfcffabce9d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filename = 'Mec'+'_w00_'\n",
        "figure_file = 'plots/' + filename + '_MA.png'\n",
        "x = [i+1 for i in range(0,len(data_w00))]\n",
        "plot_learning_curve(x, data_w00,'w00', figure_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fgp0NdW6qrKY",
        "outputId": "d6b0e44d-76d9-40a8-cf9e-4cbf655e474f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filename = 'Mec'+'_w01_'\n",
        "figure_file = 'plots/' + filename + '_MA.png'\n",
        "x = [i+1 for i in range(0,len(data_w01))]\n",
        "plot_learning_curve(x, data_w01,'w01', figure_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qMOkcaWGQd9z"
      },
      "outputs": [],
      "source": [
        "aa = data_w00[900:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdColUeh4bB4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xm0Kj5bQQpkC"
      },
      "outputs": [],
      "source": [
        "delay =[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WP3_LhCRQc-A"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i in aa:\n",
        "  delay.append(-i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "trdS1yyytFJL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "moy = 0\n",
        "for i in delay:\n",
        "  moy= moy+i\n",
        "\n",
        "moy = moy/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hbf_boAtQaqr",
        "outputId": "5ce9db8b-5ab6-4ae6-c8cd-fe6e14677060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12.212345277074496"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "moy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WHo40PIxQnEA"
      },
      "outputs": [],
      "source": [
        "actions = loadtxt('/content/actionsw01.csv', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rC-SsjLFSvoa",
        "outputId": "fe3857ea-e302-43b8-a195-c05c9a42f4c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3., 3., 3., ..., 4., 4., 4.])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OZvXON-Bzz-9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}